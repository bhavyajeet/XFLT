{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.tokenize import indic_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/muril-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import functorch\n",
    "import regex as re \n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "from nltk.util import ngrams \n",
    "from collections import Counter\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\", padding='max_length', truncation='max_length', max_length=512)\n",
    "model = AutoModel.from_pretrained(\"google/muril-base-cased\", output_hidden_states=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_duplicates(embeddings, lst, mean=True):\n",
    "    output = [None for _ in range(len(set(lst)))]\n",
    "    i = 0\n",
    "    for idx, i in enumerate(lst):\n",
    "        if(i!=None):\n",
    "            if(output[i] == None):\n",
    "                output[i] = embeddings[idx, :].reshape(1, -1)\n",
    "            else:\n",
    "                output[i] = torch.cat((output[i], embeddings[idx, :].reshape(1, -1)), dim=0)\n",
    "    if(mean):\n",
    "        for idx, val in enumerate(output):\n",
    "            output[idx] = torch.mean(output[idx], dim=0).reshape(1, -1)\n",
    "    return output\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def get_embedding(tokens, split_into_words=False):\n",
    "    #print(tokens)\n",
    "    with torch.no_grad():\n",
    "        tokenized_facts = tokenizer(tokens, padding=True, truncation=True, max_length=512, is_split_into_words=split_into_words, return_tensors=\"pt\").to(device)\n",
    "        #print(tokenizer.convert_ids_to_tokens(tokenized_facts['input_ids'][0]))\n",
    "        states = model(**tokenized_facts).hidden_states\n",
    "        output = torch.stack([states[i] for i in range(len(states))])\n",
    "        output = output.squeeze()\n",
    "        #print(output.shape)\n",
    "        final_hidden_state = torch.mean(output[:, :, ...], dim=0)\n",
    "        #final_hidden_state = output[-2, :, ...]\n",
    "        #print(final_hidden_state.shape)\n",
    "        return final_hidden_state[1:-1], tokenized_facts.word_ids()[1:-1]\n",
    "        #return embeddings['last_hidden_state'] #tokenized_facts['attention_mask']\n",
    "        #return torch.mean(embeddings['last_hidden_state'], dim=1)\n",
    "\n",
    "def lcs(X, Y):\n",
    "    m = len(X)\n",
    "    n = len(Y)\n",
    "    L = [[None]*(n + 1) for i in range(m + 1)]\n",
    "    for i in range(m + 1):\n",
    "        for j in range(n + 1):\n",
    "            if i == 0 or j == 0 :\n",
    "                L[i][j] = 0\n",
    "            elif X[i-1] == Y[j-1]:\n",
    "                L[i][j] = L[i-1][j-1]+1\n",
    "            else:\n",
    "                L[i][j] = max(L[i-1][j], L[i][j-1])\n",
    " \n",
    "    return L[m][n]\n",
    "\n",
    "def get_similarity(embedding_a, embedding_b, metric=F.cosine_similarity):\n",
    "    embedding_a = embedding_a.repeat(embedding_b.shape[0], 1, 1)\n",
    "    #print(token_embedding.shape)\n",
    "    return functorch.vmap(lambda x, y: metric(x, y))(embedding_a, embedding_b)\n",
    "\n",
    "def get_facts(source, token_split=True):\n",
    "    words = source.split(\" \")\n",
    "    #language = words[1]\n",
    "    facts = re.split(r'<[^>]*>', \" \".join(words[3:]))\n",
    "    facts = [re.sub(r'_', ' ', facts[i].strip()) for i in range(len(facts))]\n",
    "    #print(facts)\n",
    "    if(token_split):\n",
    "        return [re.sub(r'_', ' ', f) for i in range(len(facts)) for f in facts[i].split()], [i-1 for i, word in enumerate(facts) for _ in range(len(word.split()))]\n",
    "    return facts, []\n",
    "\n",
    "\n",
    "def entailment_prob(fact_embeddings, generated_ngram, threshold=None): \n",
    "    # facts = get_facts(source)\n",
    "    # fact_string = \" \".join(facts)\n",
    "    # fact_embeddings, _ = get_embedding(fact_string)\n",
    "    if(generated_ngram.dim() == 1):\n",
    "        generated_ngram = generated_ngram.reshape(1, -1)\n",
    "    similarities = functorch.vmap(lambda row_a: F.cosine_similarity(row_a, fact_embeddings))(generated_ngram)\n",
    "    if(threshold):\n",
    "        #print(torch.max(similarities, dim=1).values.cpu())\n",
    "        return np.mean((torch.max(similarities, dim=1).values > threshold).int().cpu().numpy())\n",
    "    return np.mean(torch.max(similarities, dim=1).values.cpu().numpy())\n",
    "\n",
    "def parent_precision(fact_embeddings, reference_tokens, generated_tokens, reference_embeddings, generated_embeddings, threshold=None, n_max=1):\n",
    "    # language = lang_code_map[source.split(\" \")[1]]\n",
    "    # generated = re.sub(r\"[',।]\", '', generated)\n",
    "    # reference = re.sub(r\"[',।]\", '', reference)\n",
    "    # if(language!='en'):\n",
    "    #     generated_tokens = indic_tokenizetrivial_tokenize(generated, lang=language)\n",
    "    #     reference_tokens = indic_tokenize.trivial_tokenize(reference, lang=language)\n",
    "    # else:\n",
    "    #     generated_tokens = generated.split(\" \")\n",
    "    #     reference_tokens = reference.split(\" \")\n",
    "    entailed_precisions = [] \n",
    "    for n in range(1, n_max+1):\n",
    "        generated_ngrams = Counter(list(ngrams(generated_tokens, n)))\n",
    "        reference_ngrams = Counter(list(ngrams(reference_tokens, n)))\n",
    "        ngram_intersection = generated_ngrams & reference_ngrams\n",
    "        entailed_precision = 0\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "        for ngram, count in generated_ngrams.items():\n",
    "            denominator+=count \n",
    "            ngram_embedding = torch.cat([generated_embeddings[i] for i in ngram if i in generated_embeddings]).squeeze()\n",
    "            ep = entailment_prob(fact_embeddings, ngram_embedding, threshold)\n",
    "            prob_ngram_in_ref = min(1, reference_ngrams.get(ngram, 0)/count)\n",
    "            numerator += count*(prob_ngram_in_ref + (1-prob_ngram_in_ref)*ep)\n",
    "            #entailed_precision+=(generated_ngrams[ngram]*ep)\n",
    "            #entailed_precision+=1\n",
    "            # if(ngram in ngram_intersection):\n",
    "            #     entailed_precision+=(ngram_intersection[ngram]*(1-ep))\n",
    "                #entailed_precision+=ep\n",
    "        if(denominator == 0):\n",
    "            entailed_precisions.append(0)\n",
    "        else:\n",
    "            entailed_precisions.append(numerator/denominator)\n",
    "    final = np.exp(sum(map(lambda x: np.log(x+1e-10), entailed_precisions))/n_max)\n",
    "    return min(1, final)\n",
    "\n",
    "def parent_recall(fact_embeddings, reference_tokens, generated_tokens, reference_embeddings, generated_embeddings, trade_off=0.3, threshold=None, n_max=4):\n",
    "    # language = lang_code_map[source.split(\" \")[1]]\n",
    "    # generated = re.sub(r\"[',।]\", '', generated)\n",
    "    # reference = re.sub(r\"[',।]\", '', reference)\n",
    "    \n",
    "    # if(language!='en'):\n",
    "    #     generated_tokens = indic_tokenize.trivial_tokenize(generated, lang=language)\n",
    "    #     reference_tokens = indic_tokenize.trivial_tokenize(reference, lang=language)\n",
    "    # else:\n",
    "    #     generated_tokens = generated.split(\" \")\n",
    "    #     reference_tokens = reference.split(\" \")\n",
    "    entailed_recall_reference = [] \n",
    "    for n in range(1, n_max+1):\n",
    "        generated_ngrams = Counter(list(ngrams(generated_tokens, n)))\n",
    "        reference_ngrams = Counter(list(ngrams(reference_tokens, n)))\n",
    "        ngram_intersection = generated_ngrams & reference_ngrams\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "        for ngram, count in reference_ngrams.items():\n",
    "            ngram_embedding = torch.cat([reference_embeddings[i] for i in ngram if i in reference_embeddings]).squeeze()\n",
    "            ep = entailment_prob(fact_embeddings, ngram_embedding, threshold)\n",
    "            prob_ngram_in_pred = min(1, generated_ngrams.get(ngram, 0)/count)\n",
    "            #print(ngram, ep, prob_ngram_in_pred)\n",
    "            denominator+=count*ep\n",
    "            numerator+=count*prob_ngram_in_pred*ep ## This is always very low because entailment probability is very low because stopwords like (is a) etc never match with src  ## Is not a problem in original parent because it is based on tokenwise matching, in our approach \"is\" matching is just as important as \"bangladeshi\" matching\n",
    "        if(denominator == 0):\n",
    "            entailed_recall_reference.append(1)\n",
    "        else:\n",
    "            entailed_recall_reference.append(numerator/denominator)\n",
    "    #print(entailed_recall_reference)\n",
    "    final_entailed_recall_reference = np.exp(sum(map(lambda x: np.log(x+1e-10), entailed_recall_reference))/n_max)\n",
    "    #print(\"recall-ref\", final_entailed_recall_reference)\n",
    "    # for fact in facts:\n",
    "    #     print(fact)\n",
    "    #     print(generated)\n",
    "    #     print(get_similarity(fact, generated))\n",
    "    gen_emb = torch.cat(generated_embeddings).squeeze()\n",
    "    if(gen_emb.dim() == 1):\n",
    "        gen_emb = gen_emb.reshape(1, -1)\n",
    "    similarities_gen = functorch.vmap(lambda row_a: F.cosine_similarity(row_a, fact_embeddings))(gen_emb).T\n",
    "    tokenwise_max_matches = torch.max(similarities_gen, dim=1).values\n",
    "    # print(\"fact\", fact_embeddings.shape)\n",
    "    # print(\"gen\", gen_emb.shape)\n",
    "    # print(tokenwise_max_matches)\n",
    "    #tokenwise_max_matches = [torch.max(torch.max(get_similarity(fact, generated), dim=1).values).item() for fact in facts]\n",
    "    #tokenwise_max_matches = [torch.mean((torch.max(get_similarity(fact, generated), dim=1).values > 0.5).float()).item() for fact in facts]\n",
    "    #tokenwise_max_matches = [torch.mean((torch.max(get_similarity(fact, generated), dim=0).values > 0.5).float()).item() for fact in facts]\n",
    "    #print(tokenwise_max_matches)\n",
    "    #print((tokenwise_max_matches>threshold).int().numpy())\n",
    "    entailed_recall_source_max = np.mean((tokenwise_max_matches>threshold).int().numpy())\n",
    "    #print(\"recall-source\", entailed_recall_source)\n",
    "    # fact_token_matches = []\n",
    "    # for fact in facts:\n",
    "    #     similarity = get_similarity(fact, generated)\n",
    "    #     match_idx = torch.max(similarity, dim=0).indices \n",
    "    #     fact_token_match = lcs(match_idx, range(len(match_idx)))\n",
    "    #     fact_token_matches.append(fact_token_match/len(match_idx))\n",
    "    # # print(facts)\n",
    "    # # print(fact_token_matches)\n",
    "    # #print(tokenwise_max_matches)\n",
    "\n",
    "    # entailed_recall_source_lcs = np.mean(fact_token_matches)\n",
    "    #print(entailed_recall_source)\n",
    "    #print(final_entailed_recall_reference, entailed_recall_source)\n",
    "    #recall_ref = np.power(final_entailed_recall_reference, trade_off)\n",
    "    #recall_src_lcs = np.power(entailed_recall_source_lcs, 1-trade_off)\n",
    "    #recall_src_max = np.power(entailed_recall_source_max, 1-trade_off)\n",
    "    #print(recall_ref, recall_src)\n",
    "    return final_entailed_recall_reference, entailed_recall_source_max\n",
    "\n",
    "def xparent_f1(source, reference, generated, tokenizer, trade_off=0.5):\n",
    "    #language = lang_code_map[source.split(\" \")[1]]\n",
    "    generated = re.sub(r\"[',.।()]\", '', generated)\n",
    "    reference = re.sub(r\"[',.।()]\", '', reference)\n",
    "\n",
    "    generated_tokens = tokenizer(re.sub(r'[ ]{2,}', ' ', generated.strip()))\n",
    "    reference_tokens = tokenizer(re.sub(r'[ ]{2,}', ' ', reference.strip()))\n",
    "    generated_embeddings, g_idx = get_embedding(tuple(generated_tokens), True)\n",
    "    reference_embeddings, r_idx = get_embedding(tuple(reference_tokens), True)\n",
    "\n",
    "    gen_emb = group_duplicates(generated_embeddings, g_idx)\n",
    "    ref_emb = group_duplicates(reference_embeddings, r_idx)\n",
    "\n",
    "    g_dict = {t: emb for t, emb in zip(generated_tokens, gen_emb)}\n",
    "    r_dict = {t: emb for t, emb in zip(reference_tokens, ref_emb)}\n",
    "\n",
    "    facts, fact_pos = get_facts(source, token_split=True)\n",
    "    #fact_string = \" \".join(facts)\n",
    "    #print(facts)\n",
    "    fact_embeddings, f_idx = get_embedding(tuple(facts), split_into_words=True)\n",
    "    fact_embeddings = group_duplicates(fact_embeddings, f_idx)\n",
    "    fact_emb = torch.cat(fact_embeddings).squeeze()\n",
    "    #print([facts[idx] for idx, i in enumerate(fact_pos) if i%2==0])\n",
    "    fact_emb_wo_names = torch.cat([fact_embeddings[idx] for idx, i in enumerate(fact_pos) if i%2==0]).squeeze()\n",
    "    precision = parent_precision(fact_emb, reference_tokens, generated_tokens, r_dict, g_dict, threshold=0.45, n_max=4)    \n",
    "    recall_ref, recall_src = parent_recall(fact_emb_wo_names, reference_tokens, generated_tokens, r_dict, gen_emb, threshold=0.45, n_max=4)\n",
    "    # #print(\"precisio\", precision)\n",
    "    # recall_ref, recall_src_max, recall_src_lcs = parent_recall(source, reference_tokens, generated_tokens, generated, trade_off, n_max=4)\n",
    "    # #print(\"recall\", recall)\n",
    "    return precision, recall_ref, recall_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"generate english : <H> michael dahlquist <R> date_of_birth <T> 22 december 1965 <R> place_of_birth <T> seattle washington <R> date_of_death <T> 14 july 2005 <R> place_of_death <T> stoke illinois <R> occupation <T> drummer <R> instrument <T> drums\"\n",
    "ref = \"Michael Dahlquist ( December 22, 1965 - July 14, 2005 ) was a drummer in the Seattle band Silkwork\"\n",
    "\n",
    "gen1 = \"Michael Dahlquist ( December 22, 1965 - July 14, 2005 ) was a drummer in the California band Grateful Dead\"\n",
    "gen2 = \"Michael Dahlquist ( December 22, 1965 - July 14, 2005 ) was a drummer\"\n",
    "gen3 = \"Michael Dahlquist ( December 22, 1965 - July 14, 2005 ) was a drummer from Seattle, Washington\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"generate hindi : <H> vinesh antani <R> occupation <T> novelist <R> award_received <T> sahitya akademi award <QR> point_in_time <QT> 2000 <R> languages_spoken,_written_or_signed <T> gujarati\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['vinesh',\n",
       "  'antani',\n",
       "  'occupation',\n",
       "  'novelist',\n",
       "  'award',\n",
       "  'received',\n",
       "  'sahitya',\n",
       "  'akademi',\n",
       "  'award',\n",
       "  'point',\n",
       "  'in',\n",
       "  'time',\n",
       "  '2000',\n",
       "  'languages',\n",
       "  'spoken,',\n",
       "  'written',\n",
       "  'or',\n",
       "  'signed',\n",
       "  'gujarati'],\n",
       " [0, 0, 1, 2, 3, 3, 4, 4, 4, 5, 5, 5, 6, 7, 7, 7, 7, 7, 8])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_facts(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_off = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7737647269545713 0.8905685238406152 0.7142857142857143\n",
      "0.7854877926509592\n"
     ]
    }
   ],
   "source": [
    "a, b, c = xparent_f1(src, ref, gen1, lambda x: indic_tokenize.trivial_tokenize(x))\n",
    "print(a, b, c)\n",
    "b = np.power(b, 1-trade_off)\n",
    "c = np.power(c, trade_off)\n",
    "d = b*c \n",
    "print((2*a*d)/(a+d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.8431006188348239 0.7142857142857143\n",
      "0.8738896958751808\n"
     ]
    }
   ],
   "source": [
    "a, b, c = xparent_f1(src, ref, gen2, lambda x: indic_tokenize.trivial_tokenize(x))\n",
    "print(a, b, c)\n",
    "b = np.power(b, 1-trade_off)\n",
    "c = np.power(c, trade_off)\n",
    "d = b*c \n",
    "print((2*a*d)/(a+d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.908271937013372 0.8656030553545158 0.8571428571428571\n",
      "0.8841955178419512\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "a, b, c = xparent_f1(src, ref, gen3, lambda x: indic_tokenize.trivial_tokenize(x))\n",
    "print(a, b, c)\n",
    "b = np.power(b, 1-trade_off)\n",
    "c = np.power(c, trade_off)\n",
    "d = b*c \n",
    "print((2*a*d)/(a+d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('textbox')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84fc63c8ae42b05f54f8c8e4c73411ce0404f059987aac7c448c556c45688d5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
