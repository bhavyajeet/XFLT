{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b979ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd250fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import combinations,permutations \n",
    "import numpy as np\n",
    "from sklearn import cluster\n",
    "import glob,os\n",
    "import pickle\n",
    "import warnings\n",
    "from sklearn.cluster import DBSCAN\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7584aa18",
   "metadata": {},
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/userid/Desktop/MultiSent/datasets/\"\n",
    "all_datasets = glob.glob(root + 'split_merged_dataset/*_train.jsonl')\n",
    "all_datasets\n",
    "all_datasets[3].split('/')[-1][:-6].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0651b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = []\n",
    "for dataset in all_datasets:\n",
    "    lang = dataset.split('/')[-1][:-6].split('_')[0]\n",
    "    languages.append(lang)\n",
    "print(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34b0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/userid/Desktop/MultiSent/datasets/split_data/\"\n",
    "ent_dict = defaultdict()\n",
    "fact_dict_all = defaultdict()\n",
    "\n",
    "languages = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(root):\n",
    "    print(subdir)\n",
    "    \n",
    "    lang =  subdir.split('/')[-1]\n",
    "    print(lang)\n",
    "    \n",
    "    print(files)\n",
    "    \n",
    "    e_dict = {}\n",
    "    #i=0\n",
    "    if lang != \"\":\n",
    "        with open(subdir+'/train.jsonl') as f:\n",
    "            unique_facts = []\n",
    "\n",
    "            for line in f:\n",
    "#                 if i>10:\n",
    "#                     break\n",
    "#                 #i+=1\n",
    "#                 print(i)\n",
    "            \n",
    "                data = json.loads(line)\n",
    "                f = []\n",
    "                for l in data['facts_list']:\n",
    "                    #print('sentence')\n",
    "                    f2 = []\n",
    "                    for fact in l:\n",
    "                        f2.append(fact[0])  \n",
    "                        # Generate facts \n",
    "                        unique_facts.append(fact[0])  \n",
    "                    f.append(f2)\n",
    "                    #print(f)\n",
    "                e_dict[data['entity_name']] = f\n",
    "                #i+=1\n",
    "        languages.append(lang)\n",
    "        fact_dict_all[lang] = set(unique_facts)\n",
    "        print(lang,len(fact_dict_all[lang]))\n",
    "        ent_dict[lang] = e_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c2e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd49b50a",
   "metadata": {},
   "source": [
    "#### Affinity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd0ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_dict = defaultdict()\n",
    "factsetdict = defaultdict()\n",
    "\n",
    "for lang in languages:\n",
    "    print(lang)\n",
    "    fdict = {i:f for i,f in enumerate(fact_dict_all[lang])}\n",
    "    \n",
    "    factsetdict[lang] = set(fact_dict_all[lang])\n",
    "    \n",
    "    fact_dict[lang] = fdict\n",
    "    #print(fact_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec26a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_dict = defaultdict()\n",
    "for i,(k,facts) in enumerate(factsetdict.items()):\n",
    "    perm_dict[k] = list(permutations(facts,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75b5073",
   "metadata": {},
   "source": [
    "#### PMI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4185636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/userid/Desktop/MultiSent/datasets/\"\n",
    "all_datasets = glob.glob(root + 'fact_pair_probs/*.tsv')\n",
    "all_datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51749d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_dict = defaultdict()\n",
    "for lang in languages:\n",
    "    for dataset in all_datasets:\n",
    "        #print(dataset.split('/')[-1].split('.')[0])\n",
    "        if lang == dataset.split('/')[-1].split('.')[0]:\n",
    "            print(lang)\n",
    "            pmi_list = []\n",
    "            with open(dataset) as f:\n",
    "                for line in f:\n",
    "                    l = line.split('\\t')\n",
    "                    pmi_list.append(l)\n",
    "    pmi_dict[lang] = pmi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e10da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pmi_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11fa582",
   "metadata": {},
   "source": [
    "##### Spectral clustering for all entities on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57053b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/userid/Desktop/MultiSent/datasets/split_data/\"\n",
    "ent_dict = defaultdict()\n",
    "cnts_dict = {}\n",
    "languages = []\n",
    "\n",
    "\n",
    "for subdir, dirs, files in os.walk(root):\n",
    "    print(subdir)\n",
    "    \n",
    "    lang =  subdir.split('/')[-1]\n",
    "    print(lang)\n",
    "    \n",
    "    print(files)\n",
    "    \n",
    "    e_dict = {}\n",
    "    cnts = []\n",
    "    \n",
    "    if lang != \"\":\n",
    "        with open(subdir+'/test.jsonl') as f:\n",
    "            unique_facts = []\n",
    "\n",
    "            for line in f:\n",
    "                c = 0\n",
    "                data = json.loads(line)\n",
    "                f = []\n",
    "                for l in data['facts_list']:\n",
    "                    #print('sentence')\n",
    "                    f2 = []\n",
    "                    for fact in l:\n",
    "                        f2.append(fact[0])  \n",
    "                        # Generate facts \n",
    "                        unique_facts.append(fact[0])  \n",
    "                    f.append(f2)\n",
    "                    #print(f)\n",
    "                    c+=1\n",
    "\n",
    "                cnts.append(c)\n",
    "\n",
    "                e_dict[data['entity_name']] = f\n",
    "\n",
    "        languages.append(lang)\n",
    "        #fact_dict_all[lang] = set(unique_facts)\n",
    "        print(lang,len(fact_dict_all[lang]))\n",
    "        ent_dict[lang] = e_dict\n",
    "        cnts_dict[lang] = cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312bb605",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnts_dict_ent = defaultdict()\n",
    "for i,lang in enumerate(cnts_dict):\n",
    "    num_sent = pd.Series(cnts_dict[lang]).sum()\n",
    "    #print(lang,num_sent)\n",
    "    cnts_dict_ent[lang]=num_sent\n",
    "print(cnts_dict_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b78cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(cnts_dict['te']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1228b1f",
   "metadata": {},
   "source": [
    "### Get number of sentence classifier's predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f34c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_all = pd.read_csv('/Users/userid/Desktop/MultiSent/MEMS-XF2T/dataset_prep/test_all.csv',header=None,names=['text','nsent'],index_col=None) \n",
    "test_all['lang']= test_all['text'].astype(str).str[0:2]\n",
    "test_all['index'] = test_all.index\n",
    "test_all.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df636bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = test_all['index'].groupby(test_all['lang'])\n",
    "res = {lang: list(group) for lang, group in groups}\n",
    "#res['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458103b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions from classifier\n",
    "my_file = open('/Users/userid/Desktop/MultiSent/MEMS-XF2T/dataset_prep/output.txt')\n",
    "data = my_file.read()\n",
    "pred = data.split(\"\\n\")[:-1]\n",
    "#print(pred)\n",
    "my_file.close()  \n",
    "pred = [int(i) for i in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55624f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8146292",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look up predictions\n",
    "from collections import defaultdict\n",
    "lang = list(res.keys())\n",
    "classifier_preds = {l: [] for l in lang}\n",
    "#print(d1)\n",
    "for k in res.keys():\n",
    "    for item in res[k]:\n",
    "        #print(item)\n",
    "        classifier_preds[k].append(pred[item])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e47c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier_preds['hi']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c33530",
   "metadata": {},
   "source": [
    "##### Create lang,line number,and predicted cluster mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf93267",
   "metadata": {},
   "source": [
    "### Create test dataset for  predicted clusters - one json for one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb08960",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ent_dict_pred = ent_dict\n",
    "# for e,c in zip(ent_dict.keys(),cnts):\n",
    "#     print(e,c)\n",
    "from collections import Counter\n",
    "root = \"/Users/userid/Desktop/MultiSent/datasets/affinity_matrices/\"\n",
    "\n",
    "entpred_dict_all = defaultdict()\n",
    "\n",
    "cnt =0\n",
    "nothing=0\n",
    "macro_pr = []\n",
    "macro_rec = []\n",
    "macro_pr_old = []\n",
    "macro_rec_old = []\n",
    "facts_len = 0\n",
    "weighted_acc =[]\n",
    "missed = []\n",
    "scores_dict = defaultdict()\n",
    "\n",
    "for lang in languages:\n",
    "    if lang == 'en':\n",
    "\n",
    "        scores = defaultdict()\n",
    "        missed = 0\n",
    "\n",
    "        print(lang)\n",
    "        affinity_matrix = np.load(root + lang + '_affinity_matrix.npy')\n",
    "        print(\"loaded affinity Matrix\")\n",
    "        #print(affinity_matrix.shape)\n",
    "        \n",
    "\n",
    "        entpred_dict  = defaultdict()\n",
    "\n",
    "        for i,v in enumerate(ent_dict[lang].keys()):\n",
    "            \n",
    "            cnt+=1\n",
    "#             if cnt<1000 or cnt>1000:\n",
    "#                 continue\n",
    "            print(cnt)\n",
    "            #print(v,ent_dict[v])\n",
    "            e = ent_dict[lang][v]\n",
    "            #c = len(ent_dict[lang][v])\n",
    "            \n",
    "            # Get predictions from the prediction csv using line number\n",
    "            c = classifier_preds[lang][cnt]\n",
    "            \n",
    "            \n",
    "\n",
    "#             print(\"v\",v)\n",
    "#             print(\"e\",e)\n",
    "#             print(\"c\",c)\n",
    "\n",
    "            #print(flat_list)\n",
    "\n",
    "\n",
    "            # Get uniques facts \n",
    "\n",
    "            flat_list = [item for sublist in e for item in sublist]\n",
    "\n",
    "            flat_set = set(flat_list)\n",
    "            #print(flat_set)\n",
    "            flat_ids = []\n",
    "\n",
    "            for item in flat_set:\n",
    "                for i,(key,value) in enumerate(fact_dict[lang].items()):\n",
    "                    if value==item:\n",
    "                        #print(value)\n",
    "                        flat_ids.append(key)\n",
    "            #print(flat_ids)\n",
    "\n",
    "\n",
    "            # Get ids\n",
    "            flat_list_cnts = dict(Counter(flat_list))\n",
    "            #print(flat_list_cnts)\n",
    "\n",
    "            ent = e\n",
    "            for i,value in enumerate(ent):\n",
    "                for j,subitem in enumerate(value):\n",
    "                    #print(ent[i][j])\n",
    "                    for lookupitem in flat_list_cnts.keys():\n",
    "                        #print(lookupitem)\n",
    "                        if subitem == lookupitem and flat_list_cnts[lookupitem]!=1:    \n",
    "                            ent[i][j] = tuple([ent[i][j],lookupitem + '_' + str(flat_list_cnts[lookupitem])])\n",
    "                            flat_list_cnts[lookupitem] -= 1\n",
    "                            #print(flat_list_cnts)\n",
    "                            #print(e)\n",
    "                            continue\n",
    "                    #print(type(ent[i][j]))\n",
    "                    if not isinstance(ent[i][j], tuple):\n",
    "                        #print(True)\n",
    "                        ent[i][j] = tuple([ent[i][j],ent[i][j]])\n",
    "\n",
    "\n",
    "            #print(\"Ent\",ent)\n",
    "            flat_ids = []\n",
    "            sent_id=0\n",
    "            act_list = []\n",
    "            flag=0\n",
    "            new_fact_id = 200000\n",
    "            for sent in ent:\n",
    "                for item in sent:\n",
    "                    #print(\"item[0]\",item[0])\n",
    "                    for i,(key,value) in enumerate(fact_dict[lang].items()):\n",
    "                        if value==item[0]:\n",
    "                            flag==1\n",
    "                            flat_ids.append(key)\n",
    "                            act_list.append(sent_id)\n",
    "                            break\n",
    "    #                     flat_ids.append(new_fact_id)\n",
    "    #                     new_fact_id +=1\n",
    "    #                break\n",
    "\n",
    "                sent_id +=1\n",
    "\n",
    "            #print(\"Actual Clusters\",act_list)\n",
    "            #print(act_list)\n",
    "\n",
    "            ent_perm_list = list(permutations(flat_ids,2))\n",
    "            #print(\"Flat ids\",flat_ids)\n",
    "           \n",
    "            #print(fact_dict[lang][flat_ids[0]])\n",
    "\n",
    "            ent_aff_matrix = np.zeros((len(flat_ids),len(flat_ids)))\n",
    "            #print(ent_aff_matrix)\n",
    "\n",
    "\n",
    "            #print(ent_perm_list)\n",
    "            for p in ent_perm_list:\n",
    "                #print(affinity_matrix[p[0],p[1]])\n",
    "                ent_aff_matrix[flat_ids.index(p[0]),flat_ids.index(p[1])] = affinity_matrix[p[0],p[1]]\n",
    "      \n",
    "    \n",
    "            #print(ent_aff_matrix)\n",
    "            # clusters\n",
    "            try:\n",
    "                #print(\"num cluster\",c)\n",
    "                ent_clusters = cluster.spectral_clustering(affinity=ent_aff_matrix,n_clusters=c)#.fit_predict()\n",
    "                #print(\"spectral Clusters\",ent_clusters)\n",
    "                \n",
    "#                 clustering = DBSCAN(eps=0.05, min_samples=1,leaf_size=1,p=1,algorithm='auto').fit(ent_aff_matrix)#.fit_predict()\n",
    "#                 ent_clusters = DBSCAN(eps=0.05, min_samples=1,leaf_size=1,p=1,algorithm='auto').fit_predict(ent_aff_matrix)\n",
    "#                 print(\"dbscan Clusters\",ent_clusters)\n",
    "#                 print(\"dbscan_nclusters\",len(np.unique(ent_clusters)))\n",
    "            \n",
    "                \n",
    "            except Exception:\n",
    "                #nothing +=1\n",
    "                #print(nothing)\n",
    "                #print(\"No clusters\")\n",
    "                #print(ent_aff_matrix.shape,c)\n",
    "                continue\n",
    "\n",
    "            \n",
    "            # Number of predicted clusters\n",
    "            #c = len(np.unique(clustering.labels_))\n",
    "            #print(\"No of predicted cl\",len(np.unique(clustering.labels_)))\n",
    "            # Assign predicted facts to predicted clusters   \n",
    "            \n",
    "            pred_facts = [[] for i in range(c)]\n",
    "            for index,fid in enumerate(flat_ids):                \n",
    "                fact_name = fact_dict[lang][fid]\n",
    "                pred_cluster = ent_clusters[index]\n",
    "                #print(fid,fact_name,pred_cluster)\n",
    "                pred_facts[pred_cluster].append(fact_name)\n",
    "            #print(pred_facts)\n",
    "            \n",
    "            ent_dict_pred[lang][v] = pred_facts\n",
    "            #print(e)\n",
    "            print(pred_facts)\n",
    "            # LABEL MAPPING\n",
    "#             if (pd.Series(act_list).nunique() == pd.Series(ent_clusters).nunique()):\n",
    "#                 new_clusters,cm,old_acc,old_pr,old_re,new_cm,new_acc,new_pr,new_re  = label_matching(act_list,ent_clusters)\n",
    "#             else:\n",
    "#                 #print(\"Missed entity\")\n",
    "#                 missed +=1\n",
    "#                 print(\"missed\"\n",
    "#http://localhost:8888/notebooks/src/01-spectral_clustering_main-classifier%2Bclustering.ipynb#\n",
    "#             entpred_dict[v] = new_clustexrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f9bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/userid/Desktop/MultiSent/datasets/split_data/\"\n",
    "pred_dict_final = defaultdict()\n",
    "cnts_dict = {}\n",
    "languages = []\n",
    "output_path = \"/Users/userid/Desktop/MultiSent/datasets/split_data_clusters_classifier\"\n",
    "\n",
    "for subdir, dirs, files in os.walk(root):\n",
    "    lang =  subdir.split('/')[-1]\n",
    "    print(lang)    \n",
    "     \n",
    "#     if lang != \"\":\n",
    "#         print(lang)\n",
    "#         i = 0\n",
    "    i = 0 \n",
    "    if lang == \"en\":\n",
    "        with open(subdir+'/test.jsonl') as f:\n",
    "            pred_list_all = []\n",
    "\n",
    "            unique_facts = []\n",
    "\n",
    "            for line in f:\n",
    "\n",
    "                i+=1\n",
    "                if i < 1000 or i > 1000:\n",
    "                    continue\n",
    "                print(i)\n",
    "\n",
    "                #print(i)   \n",
    "                #print(line)\n",
    "                pred_final = {}\n",
    "\n",
    "                data = json.loads(line)\n",
    "                #print(data)\n",
    "                f = []\n",
    "                data['facts'] = ent_dict_pred[lang][data['entity_name']]\n",
    "                pred_final[\"entity_name\"] = data[\"entity_name\"]\n",
    "                #pred_final[\"sentence_list\"] = data[\"sentence_list\"]\n",
    "                pred_final['facts_list'] = data[\"facts_list\"]\n",
    "                pred_final[\"facts\"] = ent_dict_pred[lang][data[\"entity_name\"]] \n",
    "#                 pred_final[\"qid\"]= data[\"qid\"]\n",
    "#                 pred_final[\"native_sentence_section_list\"]= data[\"native_sentence_section_list\"]\n",
    "#                 pred_final[\"translated_sentence_list\"]= data[\"translated_sentence_list\"]\n",
    "#                 pred_final[\"sent_index_list\"]= data[\"sent_index_list\"]\n",
    "#                 pred_final[\"sent_score_list\"]= data[\"sent_score_list\"]\n",
    "#                 pred_final[\"translated_facts_list\"]= data[\"translated_facts_list\"]\n",
    "#                 pred_final[\"coverage_score_list\"]= data[\"coverage_score_list\"]\n",
    "#                 pred_final[\"order_list\"]= data[\"order_list\"]\n",
    "#                 pred_final[\"role_spec_order_list\"]= data[\"role_spec_order_list\"]\n",
    "#                 pred_final[\"source_list\"]= data[\"source_list\"]\n",
    "#                 pred_final[\"sent_label\"]= data[\"sent_label\"]\n",
    "#                 pred_final[\"avg_coverage\"]= data[\"avg_coverage\"]\n",
    "#                 pred_final[\"coherence_scores_list\"]= data[\"coherence_scores_list\"]\n",
    "#                 pred_final[\"avg_coherence\"]= data[\"avg_coherence\"]\n",
    "#                 pred_final[\"norm_cov\"]= data[\"norm_cov\"]\n",
    "#                 pred_final[\"norm_coh\"]= data[\"norm_coh\"]\n",
    "#                 pred_final[\"compiled_score\"]= data[\"compiled_score\"]\n",
    "#                 pred_final[\"lang\"]= data[\"lang\"]\n",
    "\n",
    "\n",
    "                # Split by sentences\n",
    "                d = pred_final\n",
    "                #print(d['facts'])                \n",
    "            \n",
    "                #l = len(d['sentence_list'])\n",
    "                # Update based on number of facts list\n",
    "                l = len(d['facts'])\n",
    "                \n",
    "                #print('l ',l)\n",
    "\n",
    "#                 dict_list = []\n",
    "#                 for item in range(l):\n",
    "#                     dict_list.append(d.copy())\n",
    "#                 print(len(dict_list))\n",
    "                f_list = []\n",
    "                for facts in d['facts']:\n",
    "                    inner_list = []\n",
    "                    for f in facts:\n",
    "                        #print(item)\n",
    "                        flag=0\n",
    "                        for it in d['facts_list']:\n",
    "                            for l in it:\n",
    "                                #print(l)\n",
    "                                if l[0] == f:\n",
    "                                    #print(\"appended fact\")\n",
    "                                    #print(l)\n",
    "                                    inner_list.append(l)\n",
    "                                    flag=1\n",
    "                                    break\n",
    "                            if flag==1:\n",
    "                                break\n",
    "                    f_list.append(inner_list)\n",
    "#                     item['facts'] = f_list \n",
    "                #print(\"f_list\",f_list)\n",
    "                    \n",
    "        \n",
    "                pred_list_all = []\n",
    "                for item in f_list:\n",
    "                    #print(item)\n",
    "                    new_dict = pred_final.copy()\n",
    "                    new_dict['facts'] = item\n",
    "                    pred_list_all.append(new_dict)\n",
    "\n",
    "                #print(pred_list_all)\n",
    "        if not os.path.exists(os.path.join(output_path,lang)): \n",
    "            os.mkdir(os.path.join(output_path,lang))\n",
    "\n",
    "        #print(len(pred_list_all))\n",
    "        with open(output_path  + '/'+ lang + '/test.jsonl','a') as fp: \n",
    "            for item in pred_list_all:\n",
    "                #print(i)\n",
    "                json.dump(item,fp,ensure_ascii=False)\n",
    "                fp.write('\\n')\n",
    "            print(\"file written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790a1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'facts_list': [[['date of death', '10 October 1995', [], False], ['date of birth', '04 January 1923', [], False], ['occupation', 'politician', [], False], ['spouse', 'Damodara Menon', [], False]], [['member of political party', 'Indian National Congress', [], False], ['occupation', 'politician', [], False]]]}\n",
    "ll = d['facts_list']\n",
    "f = 'date of death'\n",
    "l = []\n",
    "for it in ll:\n",
    "    for i in it:\n",
    "        if i[0] == f:\n",
    "            l.append(i)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66a0bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'a':1,'b':[2,3,4],'c':5}\n",
    "d = pred_final\n",
    "l = len(d['b'])\n",
    "\n",
    "\n",
    "dict_list = []\n",
    "for i in range(l):\n",
    "    dict_list.append(d.copy())\n",
    "    \n",
    "   \n",
    "for index,item in enumerate(dict_list):\n",
    "    item['b'] = item['b'][index]\n",
    "    print(dict_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151afc7f",
   "metadata": {},
   "source": [
    "#### Create test dataset ends - skip the above if not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e95a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ent_dict \n",
    "# for e,c in zip(ent_dict.keys(),cnts):\n",
    "#     print(e,c)\n",
    "from collections import Counter\n",
    "root = \"/Users/userid/Desktop/MultiSent/datasets/affinity_matrices/\"\n",
    "\n",
    "entpred_dict_all = defaultdict()\n",
    "\n",
    "cnt =0\n",
    "nothing=0\n",
    "macro_pr = []\n",
    "macro_rec = []\n",
    "macro_pr_old = []\n",
    "macro_rec_old = []\n",
    "facts_len = 0\n",
    "weighted_acc =[]\n",
    "missed = []\n",
    "scores_dict = defaultdict()\n",
    "\n",
    "for lang in languages:\n",
    "    if lang == 'en':\n",
    "\n",
    "        scores = defaultdict()\n",
    "        missed = 0\n",
    "\n",
    "        print(lang)\n",
    "        affinity_matrix = np.load(root + lang + '_affinity_matrix.npy')\n",
    "        print(\"loaded affinity Matrix\")\n",
    "        print(affinity_matrix.shape)\n",
    "\n",
    "        entpred_dict  = defaultdict()\n",
    "\n",
    "        for i,v in enumerate(ent_dict[lang].keys()):\n",
    "            cnt+=1\n",
    "            if cnt>1000:\n",
    "                break\n",
    "            print(i)\n",
    "            #print(v,ent_dict[v])\n",
    "            e = ent_dict[lang][v]\n",
    "            c = len(ent_dict[lang][v])\n",
    "\n",
    "            print(\"v\",v)\n",
    "            print(\"e\",e)\n",
    "            print(\"c\",c)\n",
    "\n",
    "            #print(flat_list)\n",
    "\n",
    "\n",
    "            # Get uniques facts \n",
    "\n",
    "            flat_list = [item for sublist in e for item in sublist]\n",
    "\n",
    "            flat_set = set(flat_list)\n",
    "            #print(flat_set)\n",
    "            flat_ids = []\n",
    "\n",
    "            for item in flat_set:\n",
    "                for i,(key,value) in enumerate(fact_dict[lang].items()):\n",
    "                    if value==item:\n",
    "                        #print(value)\n",
    "                        flat_ids.append(key)\n",
    "            #print(flat_ids)\n",
    "\n",
    "\n",
    "            # Get ids\n",
    "            flat_list_cnts = dict(Counter(flat_list))\n",
    "            #print(flat_list_cnts)\n",
    "\n",
    "            ent = e\n",
    "            for i,value in enumerate(ent):\n",
    "                for j,subitem in enumerate(value):\n",
    "                    #print(ent[i][j])\n",
    "                    for lookupitem in flat_list_cnts.keys():\n",
    "                        #print(lookupitem)\n",
    "                        if subitem == lookupitem and flat_list_cnts[lookupitem]!=1:    \n",
    "                            ent[i][j] = tuple([ent[i][j],lookupitem + '_' + str(flat_list_cnts[lookupitem])])\n",
    "                            flat_list_cnts[lookupitem] -= 1\n",
    "                            #print(flat_list_cnts)\n",
    "                            #print(e)\n",
    "                            continue\n",
    "                    #print(type(ent[i][j]))\n",
    "                    if not isinstance(ent[i][j], tuple):\n",
    "                        #print(True)\n",
    "                        ent[i][j] = tuple([ent[i][j],ent[i][j]])\n",
    "\n",
    "\n",
    "            #print(\"Ent\",ent)\n",
    "            flat_ids = []\n",
    "            sent_id=0\n",
    "            act_list = []\n",
    "            flag=0\n",
    "            new_fact_id = 200000\n",
    "            for sent in ent:\n",
    "                for item in sent:\n",
    "                    #print(\"item[0]\",item[0])\n",
    "                    for i,(key,value) in enumerate(fact_dict[lang].items()):\n",
    "                        if value==item[0]:\n",
    "                            flag==1\n",
    "                            flat_ids.append(key)\n",
    "                            act_list.append(sent_id)\n",
    "                            break\n",
    "    #                     flat_ids.append(new_fact_id)\n",
    "    #                     new_fact_id +=1\n",
    "    #                break\n",
    "\n",
    "                sent_id +=1\n",
    "\n",
    "    #        print(\"Actual Clusters\",act_list)\n",
    "    #         print(act_list)\n",
    "\n",
    "            ent_perm_list = list(permutations(flat_ids,2))\n",
    "\n",
    "\n",
    "            ent_aff_matrix = np.zeros((len(flat_ids),len(flat_ids)))\n",
    "            #print(ent_aff_matrix.shape)\n",
    "\n",
    "\n",
    "            #print(ent_perm_list)\n",
    "            for p in ent_perm_list:\n",
    "                ent_aff_matrix[flat_ids.index(p[0]),flat_ids.index(p[1])] = affinity_matrix[p[0],p[1]]\n",
    "\n",
    "            #print(ent_aff_matrix)\n",
    "\n",
    "\n",
    "            # clusters\n",
    "            try:\n",
    "                #ent_clusters = cluster.spectral_clustering(affinity=ent_aff_matrix,n_clusters=c)#.fit_predict()\n",
    "                #print(\"Clusters\",ent_clusters)\n",
    "                ent_clusters = cluster.spectral_clustering(affinity=ent_aff_matrix,n_clusters=c)#.fit_predict()\n",
    "                print(\"predicted Clusters\",ent_clusters)\n",
    "            except Exception:\n",
    "                #nothing +=1\n",
    "                #print(nothing)\n",
    "                #print(\"No clusters\")\n",
    "                #print(ent_aff_matrix.shape,c)\n",
    "                continue\n",
    "\n",
    "\n",
    "            # LABEL MAPPING\n",
    "            if (pd.Series(act_list).nunique() == pd.Series(ent_clusters).nunique()):\n",
    "                new_clusters,cm,old_acc,old_pr,old_re,new_cm,new_acc,new_pr,new_re  = label_matching(act_list,ent_clusters)\n",
    "            else:\n",
    "                #print(\"Missed entity\")\n",
    "                missed +=1\n",
    "                print(\"missed\")\n",
    "\n",
    "            #print(\"New Clusters\",new_clusters)\n",
    "\n",
    "            macro_pr.append(new_pr)\n",
    "            macro_rec.append(new_re)\n",
    "\n",
    "\n",
    "            # Weighted accuracy\n",
    "            #print(new_acc)\n",
    "            weighted_acc.append(new_acc * len(flat_ids))\n",
    "            facts_len+= len(flat_ids)\n",
    "\n",
    "            macro_pr_old.append(old_pr)\n",
    "            macro_rec_old.append(old_re)\n",
    "\n",
    "\n",
    "            entpred_dict[v] = new_clusters\n",
    "\n",
    "        scores['missed_ent'] = missed\n",
    "        macro_pr_total = pd.Series(macro_pr).sum()/cnt\n",
    "        macro_rec_total = pd.Series(macro_rec).sum()/cnt\n",
    "        weighted_acc_total = pd.Series(weighted_acc).sum()/facts_len\n",
    "        macro_pr_old_total = pd.Series(macro_pr_old).sum()/cnt\n",
    "        macro_rec_old_total = pd.Series(macro_rec_old).sum()/cnt\n",
    "\n",
    "        scores['macro_pr'] = round(macro_pr_total*100,2)\n",
    "        scores['macro_rec'] = round(macro_rec_total*100,2)\n",
    "        scores['weighted_acc'] = round(weighted_acc_total*100,2)\n",
    "        scores['macro_pr_old'] = round(macro_pr_old_total*100,2)\n",
    "        scores['macro_rec_old'] = round(macro_rec_old_total*100,2)\n",
    "        print(scores)\n",
    "        scores_dict[lang] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a1b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb9479",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols  =['language','total_ent','total_sent_all','macro_pr','macro_rec','weighted_acc','macro_pr_old','macro_rec_old']\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i,lang in enumerate(scores_dict):\n",
    "    #print(lang)\n",
    "    df2 = [{'language': lang, 'total_ent':len(ent_dict[lang]),\n",
    "            'total_sent_all':cnts_dict_ent[lang],\n",
    "#           'total_sent':scores_dict[lang]['total_sent'],\n",
    "#            'total_sent_cov':round(scores_dict[lang]['total_sent']/cnts_dict_ent[lang],2),\n",
    "        'missed_ent':scores_dict[lang]['missed_ent'],\n",
    "       'macro_pr':scores_dict[lang]['macro_pr'],\n",
    "        'macro_rec':scores_dict[lang]['macro_rec'],\n",
    "        'weighted_acc':scores_dict[lang]['weighted_acc']\n",
    "        ,'macro_pr_old':scores_dict[lang]['macro_pr_old']\n",
    "        ,'macro_rec_old':scores_dict[lang]['macro_rec_old']}]\n",
    "    #print(df2)\n",
    "    df2= pd.DataFrame.from_dict(df2,orient='columns')\n",
    "    #print(df2)\n",
    "    \n",
    "    df = pd.concat([df,df2], ignore_index=True)\n",
    "    #df.append(df2)\n",
    "    \n",
    "print(df)    \n",
    "\n",
    "df.to_csv(\"../results/clustering-precisionrecall-alllang-24Jan2023.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24519fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# macro_pr = pd.Series(macro_pr).sum()/cnt\n",
    "# macro_rec = pd.Series(macro_rec).sum()/cnt\n",
    "# weighted_acc = pd.Series(weighted_acc).sum()/facts_len\n",
    "# macro_pr_old = pd.Series(macro_pr_old).sum()/cnt\n",
    "# macro_rec_old = pd.Series(macro_rec_old).sum()/cnt\n",
    "# print(\"Macro Precision\",(macro_pr,macro_pr_old))\n",
    "# print(\"Macro Recall\",(macro_rec,macro_rec_old))\n",
    "# print(\"Weighted_acc\",weighted_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae191a10",
   "metadata": {},
   "source": [
    "##### Store validation cluster predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entpred_dict_all['te']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609edb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "entpred_dict_all.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceedadeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/userid/Desktop/MultiSent/datasets/cluster_predictions/\"\n",
    "with open(root + 'cluster_predictions_all.pkl', 'wb') as f:\n",
    "    pickle.dump(entpred_dict_all, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4843238c",
   "metadata": {},
   "source": [
    "##### Accuracy on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf86f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(entpred_dict_all['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d614591",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973619a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols  =['language','total_ent','total_sent_all','macro_pr','macro_rec','weighted_acc','macro_pr_old','macro_rec_old']\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i,lang in enumerate(scores_dict):\n",
    "    #print(lang)\n",
    "    df2 = [{'language': lang, 'total_ent':len(ent_dict[lang]),\n",
    "            'total_sent_all':cnts_dict_ent[lang],\n",
    "#           'total_sent':scores_dict[lang]['total_sent'],\n",
    "#            'total_sent_cov':round(scores_dict[lang]['total_sent']/cnts_dict_ent[lang],2),\n",
    "        'missed_ent':scores_dict[lang]['missed_ent'],\n",
    "       'macro_pr':scores_dict[lang]['macro_pr'],\n",
    "        'macro_rec':scores_dict[lang]['macro_rec'],\n",
    "        'weighted_acc':scores_dict[lang]['weighted_acc']\n",
    "        ,'macro_pr_old':scores_dict[lang]['macro_pr_old']\n",
    "        ,'macro_rec_old':scores_dict[lang]['macro_rec_old']}]\n",
    "    #print(df2)\n",
    "    df2= pd.DataFrame.from_dict(df2,orient='columns')\n",
    "    #print(df2)\n",
    "    \n",
    "    df = pd.concat([df,df2], ignore_index=True)\n",
    "    #df.append(df2)\n",
    "    \n",
    "print(df)    \n",
    "\n",
    "df.to_csv(\"../results/clustering-precisionrecall-alllang-24Jan2023.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1895234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56e572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../results/clustering-scores-alllang-24Jan2022.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
