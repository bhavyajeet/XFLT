02/08/2023 02:10:09 AM [CRITICAL] test: merging the 12 ['as', 'bn', 'en', 'gu', 'hi', 'kn', 'ml', 'mr', 'or', 'pa', 'ta', 'te'] different languages dataset
02/08/2023 02:10:09 AM [INFO] test dataset is already present.
02/08/2023 02:10:11 AM [CRITICAL] test : script unification to Devanagari is enabled.
02/08/2023 02:10:11 AM [INFO] test dataset count : 10216
Testing: 0it [00:00, ?it/s]odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])
/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Traceback (most recent call last):
  File "/home/bhavyajeet.singh/msme/MEMS-XF2T/generation/mT5-baseline/main.py", line 580, in <module>
    start_training(args)
  File "/home/bhavyajeet.singh/msme/MEMS-XF2T/generation/mT5-baseline/main.py", line 426, in start_training
    trainer.test(model=model, ckpt_path=checkpoint_file)
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 719, in test
    results = self.__test_given_model(model, test_dataloaders)
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 784, in __test_given_model
    results = self.fit(model)
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 445, in fit
    results = self.accelerator_backend.train()
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 148, in train
    results = self.ddp_train(process_idx=self.task_idx, model=model)
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 282, in ddp_train
    results = self.train_or_test()
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 64, in train_or_test
    results = self.trainer.run_test()
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 628, in run_test
    eval_loop_results, _ = self.run_evaluation(test_mode=True)
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 579, in run_evaluation
    output = self.evaluation_loop.evaluation_step(test_mode, batch, batch_idx, dataloader_idx)
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 169, in evaluation_step
    output = self.trainer.accelerator_backend.test_step(args)
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 166, in test_step
    output = self.training_step(args)
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 158, in training_step
    output = self.trainer.model(*args)
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py", line 179, in forward
    output = self.module.test_step(*inputs[0], **kwargs[0])
  File "/home/bhavyajeet.singh/msme/MEMS-XF2T/generation/mT5-baseline/main.py", line 271, in test_step
    return self._step(batch, 'test')
  File "/home/bhavyajeet.singh/msme/MEMS-XF2T/generation/mT5-baseline/main.py", line 172, in _step
    return_map.update(self._generative_step(batch))
  File "/home/bhavyajeet.singh/msme/MEMS-XF2T/generation/mT5-baseline/main.py", line 134, in _generative_step
    tokenizer = self.tokenizer
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 26, in decorate_context
    return func(*args, **kwargs)
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/transformers/generation_utils.py", line 1070, in generate
    **model_kwargs,
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/transformers/generation_utils.py", line 1798, in beam_search
    strs = tokenizer.batch_decode(input_ids, skip_special_tokens=True)
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/transformers/tokenization_utils_base.py", line 3133, in batch_decode
    for seq in sequences
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/transformers/tokenization_utils_base.py", line 3133, in <listcomp>
    for seq in sequences
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/transformers/tokenization_utils_base.py", line 3169, in decode
    **kwargs,
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/transformers/tokenization_utils.py", line 761, in _decode
    sub_texts.append(self.convert_tokens_to_string(current_sub_text))
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5.py", line 286, in convert_tokens_to_string
    if token in self.all_special_tokens:
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/transformers/tokenization_utils_base.py", line 1232, in all_special_tokens
    all_toks = [str(s) for s in self.all_special_tokens_extended]
  File "/home/bhavyajeet.singh/anaconda3/envs/xalign/lib/python3.7/site-packages/transformers/tokenization_utils_base.py", line 1232, in <listcomp>
    all_toks = [str(s) for s in self.all_special_tokens_extended]
KeyboardInterrupt