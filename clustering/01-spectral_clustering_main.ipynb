{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b979ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd250fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import combinations,permutations \n",
    "import numpy as np\n",
    "from sklearn import cluster\n",
    "import glob\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7584aa18",
   "metadata": {},
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f1f598a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ta'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = \"/Users/rahulmehta/Desktop/MultiSent/datasets/\"\n",
    "all_datasets = glob.glob(root + 'split_merged_dataset/*_train.jsonl')\n",
    "all_datasets\n",
    "all_datasets[3].split('/')[-1][:-6].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0651b7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pa', 'gu', 'hi', 'ta', 'as', 'ml', 'te', 'kn', 'bn', 'or', 'mr', 'en']\n"
     ]
    }
   ],
   "source": [
    "languages = []\n",
    "for dataset in all_datasets:\n",
    "    lang = dataset.split('/')[-1][:-6].split('_')[0]\n",
    "    languages.append(lang)\n",
    "print(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "774dcc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pa\n",
      "pa 218\n",
      "gu\n",
      "gu 133\n",
      "hi\n",
      "hi 216\n",
      "ta\n",
      "ta 225\n",
      "as\n",
      "as 136\n",
      "ml\n",
      "ml 244\n",
      "te\n",
      "te 166\n",
      "kn\n",
      "kn 165\n",
      "bn\n",
      "bn 245\n",
      "or\n",
      "or 141\n",
      "mr\n",
      "mr 189\n",
      "en\n",
      "en 301\n"
     ]
    }
   ],
   "source": [
    "ent_dict = defaultdict()\n",
    "fact_dict_all = defaultdict()\n",
    "\n",
    "languages = []\n",
    "for dataset in all_datasets:\n",
    "    lang = dataset.split('/')[-1][:-6].split('_')[0]\n",
    "    \n",
    "    e_dict = {}\n",
    "    print(lang)\n",
    "    with open(dataset) as f:\n",
    "        \n",
    "        \n",
    "        unique_facts = []\n",
    "\n",
    "        for line in f:\n",
    "    #         if i>50:\n",
    "    #             break\n",
    "            #print(i)\n",
    "            data = json.loads(line)\n",
    "            f = []\n",
    "            for l in data['facts_list']:\n",
    "                #print('sentence')\n",
    "                f2 = []\n",
    "                for fact in l:\n",
    "                    f2.append(fact[0])  \n",
    "                    # Generate facts \n",
    "                    unique_facts.append(fact[0])  \n",
    "                f.append(f2)\n",
    "                #print(f)\n",
    "            e_dict[data['entity_name']] = f\n",
    "            i+=1\n",
    "    languages.append(lang)\n",
    "    fact_dict_all[lang] = set(unique_facts)\n",
    "    print(lang,len(fact_dict_all[lang]))\n",
    "    ent_dict[lang] = e_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd49b50a",
   "metadata": {},
   "source": [
    "#### Affinity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "1969c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dd0ffe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pa\n",
      "gu\n",
      "hi\n",
      "ta\n",
      "as\n",
      "ml\n",
      "te\n",
      "kn\n",
      "bn\n",
      "or\n",
      "mr\n",
      "en\n"
     ]
    }
   ],
   "source": [
    "fact_dict = defaultdict()\n",
    "factsetdict = defaultdict()\n",
    "\n",
    "for lang in languages:\n",
    "    print(lang)\n",
    "    fdict = {i:f for i,f in enumerate(fact_dict_all[lang])}\n",
    "    \n",
    "    factsetdict[lang] = set(fact_dict_all[lang])\n",
    "    \n",
    "    fact_dict[lang] = fdict\n",
    "    #print(fact_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ec26a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_dict = defaultdict()\n",
    "for i,(k,facts) in enumerate(factsetdict.items()):\n",
    "    perm_dict[k] = list(permutations(facts,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75b5073",
   "metadata": {},
   "source": [
    "#### PMI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fd21f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pd.read_csv('../datasets/fact_pair_probs/en.tsv',sep='\\t',header=None)\n",
    "# data.columns = ['relation','scores']\n",
    "# data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4185636a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/rahulmehta/Desktop/MultiSent/datasets/fact_pair_probs/ta.tsv',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/fact_pair_probs/bn.tsv',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/fact_pair_probs/mr.tsv',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/fact_pair_probs/te.tsv',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/fact_pair_probs/as.tsv',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/fact_pair_probs/gu.tsv',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/fact_pair_probs/hi.tsv',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/fact_pair_probs/en.tsv',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/fact_pair_probs/pa.tsv',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/fact_pair_probs/kn.tsv',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/fact_pair_probs/or.tsv',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/fact_pair_probs/ml.tsv']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = \"/Users/rahulmehta/Desktop/MultiSent/datasets/\"\n",
    "all_datasets = glob.glob(root + 'fact_pair_probs/*.tsv')\n",
    "all_datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "b9e0ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_dict = defaultdict()\n",
    "for lang in languages:\n",
    "    for dataset in all_datasets:\n",
    "        if lang == dataset.split('/')[-1][:-6].split('_')[0]:\n",
    "            pmi_list = []\n",
    "            with open(dataset) as f:\n",
    "                for line in f:\n",
    "                    l = line.split('\\t')\n",
    "                    pmi_list.append(l)\n",
    "    pmi_dict[lang] = pmi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "9e10da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pmi_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7d1ab",
   "metadata": {},
   "source": [
    "#### Affinity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd7e44f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'perm_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [40], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(lang)\n\u001b[1;32m      4\u001b[0m a_dict \u001b[38;5;241m=\u001b[39m defaultdict()\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[43mperm_dict\u001b[49m[lang]:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#print(p)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     look \u001b[38;5;241m=\u001b[39m p[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m  p[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      8\u001b[0m     look2 \u001b[38;5;241m=\u001b[39m p[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m  p[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'perm_dict' is not defined"
     ]
    }
   ],
   "source": [
    "affinity_dict = defaultdict()\n",
    "for lang in languages: \n",
    "    print(lang)\n",
    "    a_dict = defaultdict()\n",
    "    for p in perm_dict[lang]:\n",
    "        #print(p)\n",
    "        look = p[0] + '|' +  p[1]\n",
    "        look2 = p[1] + '|' +  p[0]\n",
    "        for item in pmi_dict[lang]:\n",
    "            if look == item[0] or look2 == item[0]:\n",
    "                # print(\"yes\")\n",
    "                #print(p)\n",
    "                a_dict[p] = float(item[1].split('\\n')[:-1][0])\n",
    "                break\n",
    "            else:\n",
    "                a_dict[p] = float(0)\n",
    "    affinity_dict[lang] = a_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa73612e",
   "metadata": {},
   "source": [
    "#### Create affinity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c06fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "e4f77b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pa\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "gu\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "hi\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "ta\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "as\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "ml\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "te\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "kn\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "bn\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "or\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "mr\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "en\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'en'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [572], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m cols \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(fact_dict[lang]\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m fact_dict[lang]\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43maffinity_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     13\u001b[0m        \u001b[38;5;66;03m# print(item)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m#print(r,item[0])\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fact_dict[lang][r] \u001b[38;5;241m==\u001b[39m item[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m fact_dict[lang][c] \u001b[38;5;241m==\u001b[39m item[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;66;03m#print(\"yes\")\u001b[39;00m\n\u001b[1;32m     17\u001b[0m             affinity_matrix[r][c] \u001b[38;5;241m=\u001b[39m affinity_dict[lang][item]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'en'"
     ]
    }
   ],
   "source": [
    "for lang in languages:\n",
    "    print(lang)\n",
    "    if lang=='en':\n",
    "    s = len(fact_dict[lang]),len(fact_dict[lang])\n",
    "    affinity_matrix  = np.zeros(s)\n",
    "  \n",
    "    for r in fact_dict[lang].keys():\n",
    "        print(r)\n",
    "#        print(r)\n",
    "        cols = np.zeros(len(fact_dict[lang].keys()))\n",
    "        for c in fact_dict[lang].keys():\n",
    "            for item in affinity_dict[lang].keys():\n",
    "               # print(item)\n",
    "                #print(r,item[0])\n",
    "                if fact_dict[lang][r] == item[0] and fact_dict[lang][c] == item[1]:\n",
    "                    #print(\"yes\")\n",
    "                    affinity_matrix[r][c] = affinity_dict[lang][item]\n",
    "                    affinity_matrix[c][r] = affinity_dict[lang][item]\n",
    "\n",
    "    np.save(f'../datasets/affinity_matrices/{lang}_affinity_matrix.npy',affinity_matrix)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11fa582",
   "metadata": {},
   "source": [
    "##### Spectral clustering for all entities on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e0c6a5b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/rahulmehta/Desktop/MultiSent/datasets/split_merged_dataset/te_merged_test.jsonl',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/split_merged_dataset/gu_merged_test.jsonl',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/split_merged_dataset/as_merged_test.jsonl',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/split_merged_dataset/or_merged_test.jsonl',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/split_merged_dataset/ta_merged_test.jsonl',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/split_merged_dataset/bn_merged_test.jsonl',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/split_merged_dataset/hi_merged_test.jsonl',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/split_merged_dataset/pa_merged_test.jsonl',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/split_merged_dataset/en_merged_test.jsonl',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/split_merged_dataset/mr_merged_test.jsonl',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/split_merged_dataset/ml_merged_test.jsonl',\n",
       " '/Users/rahulmehta/Desktop/MultiSent/datasets/split_merged_dataset/kn_merged_test.jsonl']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = \"/Users/rahulmehta/Desktop/MultiSent/datasets/split_merged_dataset/\"\n",
    "all_datasets = glob.glob(root  + '*_test.jsonl')\n",
    "all_datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "066e6f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_dataset = '/Users/rahulmehta/Desktop/MultiSent/datasets/split_merged_dataset/' + lang + '_merged_test.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1d3d0c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te\n",
      "te 166\n",
      "gu\n",
      "gu 133\n",
      "as\n",
      "as 136\n",
      "or\n",
      "or 141\n",
      "ta\n",
      "ta 225\n",
      "bn\n",
      "bn 245\n",
      "hi\n",
      "hi 216\n",
      "pa\n",
      "pa 218\n",
      "en\n",
      "en 301\n",
      "mr\n",
      "mr 189\n",
      "ml\n",
      "ml 244\n",
      "kn\n",
      "kn 165\n"
     ]
    }
   ],
   "source": [
    "ent_dict = defaultdict()\n",
    "#fact_dict_all = defaultdict()\n",
    "cnts_dict = {}\n",
    "languages = []\n",
    "\n",
    "languages = []\n",
    "for dataset in all_datasets:\n",
    "    lang = dataset.split('/')[-1][:-6].split('_')[0]\n",
    "    print(lang)\n",
    "    \n",
    "    e_dict = {}\n",
    "    \n",
    "    cnts = []\n",
    "    \n",
    "    with open(dataset) as f:\n",
    "        \n",
    "        \n",
    "        unique_facts = []\n",
    "\n",
    "        for line in f:\n",
    "            c = 0\n",
    "            data = json.loads(line)\n",
    "            f = []\n",
    "            for l in data['facts_list']:\n",
    "                #print('sentence')\n",
    "                f2 = []\n",
    "                for fact in l:\n",
    "                    f2.append(fact[0])  \n",
    "                    # Generate facts \n",
    "                    unique_facts.append(fact[0])  \n",
    "                f.append(f2)\n",
    "                #print(f)\n",
    "                c+=1\n",
    "            \n",
    "            cnts.append(c)\n",
    "    \n",
    "            e_dict[data['entity_name']] = f\n",
    "        \n",
    "    languages.append(lang)\n",
    "    #fact_dict_all[lang] = set(unique_facts)\n",
    "    print(lang,len(fact_dict_all[lang]))\n",
    "    ent_dict[lang] = e_dict\n",
    "    cnts_dict[lang] = cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "312bb605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'te': 2459, 'gu': 706, 'as': 818, 'or': 1691, 'ta': 6548, 'bn': 13285, 'hi': 5750, 'pa': 4024, 'en': 18003, 'mr': 2506, 'ml': 6343, 'kn': 1760})\n"
     ]
    }
   ],
   "source": [
    "cnts_dict_ent = defaultdict()\n",
    "for i,lang in enumerate(cnts_dict):\n",
    "    num_sent = pd.Series(cnts_dict[lang]).sum()\n",
    "    #print(lang,num_sent)\n",
    "    cnts_dict_ent[lang]=num_sent\n",
    "print(cnts_dict_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f90b78cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     134\n",
       "1     127\n",
       "3     109\n",
       "4      84\n",
       "5      65\n",
       "6      48\n",
       "7      37\n",
       "8      26\n",
       "9      19\n",
       "10     15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(cnts_dict['te']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b8ebdd",
   "metadata": {},
   "source": [
    "##### Clustering on a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54a90d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fact_dict = defaultdict()\n",
    "# factsetdict = defaultdict()\n",
    "\n",
    "# for lang in languages:\n",
    "#     print(lang)\n",
    "#     fdict = {i:f for i,f in enumerate(fact_dict_all[lang])}\n",
    "    \n",
    "#     factsetdict[lang] = set(fact_dict_all[lang])\n",
    "    \n",
    "#     fact_dict[lang] = fdict\n",
    "#     #print(fact_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1311da7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fact_dict['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0146fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fact_dict['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "b78a509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fact_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78d9f9f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #ent_dict \n",
    "# # for e,c in zip(ent_dict.keys(),cnts):\n",
    "# #     print(e,c)\n",
    "# root = \"/Users/rahulmehta/Desktop/MultiSent/datasets/affinity_matrices/\"\n",
    "\n",
    "# entpred_dict_all = defaultdict()\n",
    "\n",
    "# cnt =0\n",
    "# nothing=0\n",
    "# for lang in languages:\n",
    "#     if lang == 'en':\n",
    "       \n",
    "#         affinity_matrix = np.load(root + lang + '_affinity_matrix.npy')\n",
    "#         print(\"loaded affinity Matrix\")\n",
    "\n",
    "#         entpred_dict  = defaultdict()\n",
    "\n",
    "#         for i,v in enumerate(ent_dict[lang].keys()):\n",
    "#             print(lang)\n",
    "#             cnt+=1\n",
    "#             if cnt>10:\n",
    "#                 break\n",
    "#             #print(i)\n",
    "#             #print(v,ent_dict[v])\n",
    "#             e = ent_dict[lang][v]\n",
    "#             c = cnts_dict[lang][i]\n",
    "           \n",
    "#             print(v,e,c)\n",
    "#             #print(c)\n",
    "\n",
    "#             flat_list = [item for sublist in e for item in sublist]\n",
    "#             flat_set = set(flat_list)\n",
    "#             #print(flat_set)\n",
    "#             flat_ids = []\n",
    "#             #print(flat_list)\n",
    "\n",
    "#             for item in flat_set:\n",
    "#                 for i,(key,value) in enumerate(fact_dict[lang].items()):\n",
    "#                     if value==item:\n",
    "#                         #print(value)\n",
    "#                         flat_ids.append(key)\n",
    "#             #print(flat_ids)\n",
    "#             ent_perm_list = list(permutations(flat_ids,2))\n",
    "\n",
    "#             ent_aff_matrix = np.zeros((len(flat_ids),len(flat_ids)))\n",
    "#             #print(ent_aff_matrix.shape)\n",
    "\n",
    "\n",
    "#             for p in ent_perm_list:\n",
    "#                 ent_aff_matrix[flat_ids.index(p[0]),flat_ids.index(p[1])] = affinity_matrix[p[0],p[1]]\n",
    "\n",
    "#             #print(ent_aff_matrix)\n",
    "\n",
    "\n",
    "#             # Predicted clusters\n",
    "#             try:\n",
    "#                 #ent_clusters = cluster.spectral_clustering(affinity=ent_aff_matrix,n_clusters=c)#.fit_predict()\n",
    "#                 #print(\"Clusters\",ent_clusters)\n",
    "#                 ent_clusters = cluster.spectral_clustering(affinity=ent_aff_matrix,n_clusters=c)#.fit_predict()\n",
    "#                 print(\"predicted Clusters\",ent_clusters)\n",
    "#             except Exception:\n",
    "#                 #nothing +=1\n",
    "#                 #print(nothing)\n",
    "#                 #print(\"No clusters\")\n",
    "#                 #print(ent_aff_matrix.shape,c)\n",
    "#                 continue\n",
    "\n",
    "                \n",
    "#             # Group facts by clusters\n",
    "#             d = defaultdict()\n",
    "#             for i,e in zip(flat_set,ent_clusters):\n",
    "#                 d[i]=e\n",
    "#             print(d)\n",
    "\n",
    "#             x = defaultdict(list)\n",
    "            \n",
    "#             for key, value in sorted(d.items()):\n",
    "#                 x[value].append(key)\n",
    "\n",
    "#             pred_list = list(x.values())\n",
    "#             if len(pred_list) != c:\n",
    "#                 print(\"yes\")\n",
    "#             print(pred_list)\n",
    "        \n",
    "#             entpred_dict[v] = pred_list\n",
    "\n",
    "\n",
    "#         entpred_dict_all[lang] = entpred_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df25d75",
   "metadata": {},
   "source": [
    "#### Label Matching on clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "86d230b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# root = \"/Users/rahulmehta/Desktop/MultiSent/datasets/affinity_matrices/\"\n",
    "\n",
    "# cnt =0\n",
    "# nothing=0\n",
    "# languages = ['en']\n",
    "# for lang in languages:\n",
    "#     if lang == 'en':\n",
    "#         print(lang)\n",
    "#         cnt+=1\n",
    "#         if cnt>10:\n",
    "#             break\n",
    "#         affinity_matrix = np.load(root + lang + '_affinity_matrix.npy')\n",
    "#         print(\"loaded affinity Matrix\")\n",
    "#         print(affinity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "16000bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fact_dict['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d24498eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #ent_dict \n",
    "# # for e,c in zip(ent_dict.keys(),cnts):\n",
    "# #     print(e,c)\n",
    "# root = \"/Users/rahulmehta/Desktop/MultiSent/datasets/affinity_matrices/\"\n",
    "\n",
    "# entpred_dict_all = defaultdict()\n",
    "\n",
    "# cnt =0\n",
    "# nothing=0\n",
    "# for lang in languages:\n",
    "#     if lang == 'en':\n",
    "#         print(lang)\n",
    "#         cnt+=1\n",
    "#         if cnt>10:\n",
    "#             break\n",
    "#         affinity_matrix = np.load(root + lang + '_affinity_matrix.npy')\n",
    "#         print(\"loaded affinity Matrix\")\n",
    "\n",
    "\n",
    "#         entpred_dict  = defaultdict()\n",
    "\n",
    "#         for i,v in enumerate(ent_dict[lang].keys()):\n",
    "#             #print(i)\n",
    "#             #print(v,ent_dict[v])\n",
    "#             e = ent_dict[lang][v]\n",
    "#             c = cnts_dict[lang][i]\n",
    "           \n",
    "#             print(\"v\",v)\n",
    "#             print(\"e\",e)\n",
    "#             print(\"c\",c)\n",
    "            \n",
    "#             #print(flat_list)\n",
    "\n",
    "            \n",
    "#             # Get uniques facts \n",
    "            \n",
    "#             flat_list = [item for sublist in e for item in sublist]\n",
    "#             flat_set = set(flat_list)\n",
    "#             print(flat_set)\n",
    "#             flat_ids = []\n",
    "            \n",
    "#             for item in flat_set:\n",
    "#                 for i,(key,value) in enumerate(fact_dict[lang].items()):\n",
    "#                     if value==item:\n",
    "#                         #print(value)\n",
    "#                         flat_ids.append(key)\n",
    "#             print(flat_ids)\n",
    "            \n",
    "            \n",
    "#             # Get ids\n",
    "#             for \n",
    "            \n",
    "            \n",
    "            \n",
    "#             ent_perm_list = list(permutations(flat_ids,2))\n",
    "            \n",
    "\n",
    "#             ent_aff_matrix = np.zeros((len(flat_ids),len(flat_ids)))\n",
    "#             #print(ent_aff_matrix.shape)\n",
    "\n",
    "\n",
    "#             for p in ent_perm_list:\n",
    "#                 ent_aff_matrix[flat_ids.index(p[0]),flat_ids.index(p[1])] = affinity_matrix[p[0],p[1]]\n",
    "\n",
    "#             print(ent_aff_matrix)\n",
    "\n",
    "\n",
    "#             # clusters\n",
    "#             try:\n",
    "#                 #ent_clusters = cluster.spectral_clustering(affinity=ent_aff_matrix,n_clusters=c)#.fit_predict()\n",
    "#                 #print(\"Clusters\",ent_clusters)\n",
    "#                 ent_clusters = cluster.spectral_clustering(affinity=ent_aff_matrix,n_clusters=c)#.fit_predict()\n",
    "#                 print(\"predicted Clusters\",ent_clusters)\n",
    "#             except Exception:\n",
    "#                 #nothing +=1\n",
    "#                 #print(nothing)\n",
    "#                 #print(\"No clusters\")\n",
    "#                 #print(ent_aff_matrix.shape,c)\n",
    "#                 continue\n",
    "\n",
    "#             # Group facts by clusters\n",
    "#             d = defaultdict()\n",
    "#             for i,e in zip(flat_set,ent_clusters):\n",
    "#                 d[i]=e\n",
    "#             print(d)\n",
    "\n",
    "#             x = defaultdict(list)\n",
    "            \n",
    "#             for key, value in sorted(d.items()):\n",
    "#                 x[value].append(key)\n",
    "\n",
    "#             pred_list = list(x.values())\n",
    "#             if len(pred_list) != c:\n",
    "#                 print(\"yes\")\n",
    "#             #print(v)\n",
    "\n",
    "#             entpred_dict[v] = pred_list\n",
    "\n",
    "\n",
    "#         entpred_dict_all[lang] = entpred_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfac642",
   "metadata": {},
   "source": [
    "#### New logic for cluster matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "41f0fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if fact_dict['en'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "887bfb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['te', 'gu', 'as', 'or', 'ta', 'bn', 'hi', 'pa', 'en', 'mr', 'ml', 'kn']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "73e949d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from labelmapping import label_matching,make_cost_matrix,translate_clustering,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ec7e95a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te\n",
      "loaded affinity Matrix\n",
      "defaultdict(None, {'missed_ent': 0, 'macro_pr': 79.16, 'macro_rec': 78.4, 'weighted_acc': 64.64, 'macro_pr_old': 44.66, 'macro_rec_old': 44.55})\n",
      "gu\n",
      "loaded affinity Matrix\n",
      "missed\n",
      "missed\n",
      "defaultdict(None, {'missed_ent': 2, 'macro_pr': 79.34, 'macro_rec': 78.71, 'weighted_acc': 64.81, 'macro_pr_old': 46.04, 'macro_rec_old': 45.88})\n",
      "as\n",
      "loaded affinity Matrix\n",
      "missed\n",
      "missed\n",
      "defaultdict(None, {'missed_ent': 2, 'macro_pr': 79.24, 'macro_rec': 78.73, 'weighted_acc': 64.88, 'macro_pr_old': 44.76, 'macro_rec_old': 44.75})\n",
      "or\n",
      "loaded affinity Matrix\n",
      "missed\n",
      "defaultdict(None, {'missed_ent': 1, 'macro_pr': 79.16, 'macro_rec': 78.45, 'weighted_acc': 64.99, 'macro_pr_old': 42.99, 'macro_rec_old': 42.87})\n",
      "ta\n",
      "loaded affinity Matrix\n",
      "missed\n",
      "defaultdict(None, {'missed_ent': 1, 'macro_pr': 82.46, 'macro_rec': 81.94, 'weighted_acc': 67.21, 'macro_pr_old': 51.74, 'macro_rec_old': 51.66})\n",
      "bn\n",
      "loaded affinity Matrix\n",
      "missed\n",
      "defaultdict(None, {'missed_ent': 1, 'macro_pr': 78.55, 'macro_rec': 77.6, 'weighted_acc': 63.1, 'macro_pr_old': 46.93, 'macro_rec_old': 46.8})\n",
      "hi\n",
      "loaded affinity Matrix\n",
      "missed\n",
      "missed\n",
      "defaultdict(None, {'missed_ent': 2, 'macro_pr': 79.54, 'macro_rec': 78.76, 'weighted_acc': 64.15, 'macro_pr_old': 49.77, 'macro_rec_old': 49.72})\n",
      "pa\n",
      "loaded affinity Matrix\n",
      "missed\n",
      "defaultdict(None, {'missed_ent': 1, 'macro_pr': 79.19, 'macro_rec': 78.49, 'weighted_acc': 63.95, 'macro_pr_old': 49.98, 'macro_rec_old': 49.94})\n",
      "en\n",
      "loaded affinity Matrix\n",
      "missed\n",
      "missed\n",
      "missed\n",
      "missed\n",
      "missed\n",
      "defaultdict(None, {'missed_ent': 5, 'macro_pr': 80.32, 'macro_rec': 79.58, 'weighted_acc': 64.98, 'macro_pr_old': 55.54, 'macro_rec_old': 55.51})\n",
      "mr\n",
      "loaded affinity Matrix\n",
      "defaultdict(None, {'missed_ent': 0, 'macro_pr': 80.72, 'macro_rec': 80.03, 'weighted_acc': 65.5, 'macro_pr_old': 56.64, 'macro_rec_old': 56.63})\n",
      "ml\n",
      "loaded affinity Matrix\n",
      "missed\n",
      "missed\n",
      "missed\n",
      "defaultdict(None, {'missed_ent': 3, 'macro_pr': 80.51, 'macro_rec': 79.83, 'weighted_acc': 65.33, 'macro_pr_old': 55.96, 'macro_rec_old': 55.91})\n",
      "kn\n",
      "loaded affinity Matrix\n",
      "missed\n",
      "missed\n",
      "missed\n",
      "missed\n",
      "missed\n",
      "defaultdict(None, {'missed_ent': 5, 'macro_pr': 80.43, 'macro_rec': 79.76, 'weighted_acc': 65.27, 'macro_pr_old': 55.73, 'macro_rec_old': 55.7})\n"
     ]
    }
   ],
   "source": [
    "#ent_dict \n",
    "# for e,c in zip(ent_dict.keys(),cnts):\n",
    "#     print(e,c)\n",
    "from collections import Counter\n",
    "root = \"/Users/rahulmehta/Desktop/MultiSent/datasets/affinity_matrices/\"\n",
    "\n",
    "entpred_dict_all = defaultdict()\n",
    "\n",
    "cnt =0\n",
    "nothing=0\n",
    "macro_pr = []\n",
    "macro_rec = []\n",
    "macro_pr_old = []\n",
    "macro_rec_old = []\n",
    "facts_len = 0\n",
    "weighted_acc =[]\n",
    "missed = []\n",
    "scores_dict = defaultdict()\n",
    "\n",
    "for lang in languages:\n",
    "    #if lang == 'te':\n",
    "\n",
    "    scores = defaultdict()\n",
    "    missed = 0\n",
    "\n",
    "    print(lang)\n",
    "    affinity_matrix = np.load(root + lang + '_affinity_matrix.npy')\n",
    "    print(\"loaded affinity Matrix\")\n",
    "\n",
    "\n",
    "    entpred_dict  = defaultdict()\n",
    "\n",
    "    for i,v in enumerate(ent_dict[lang].keys()):\n",
    "\n",
    "        cnt+=1\n",
    "#             if cnt>10:\n",
    "#                 break\n",
    "#        print(i)\n",
    "        #print(v,ent_dict[v])\n",
    "        e = ent_dict[lang][v]\n",
    "        c = len(ent_dict[lang][v])\n",
    "\n",
    "#         print(\"v\",v)\n",
    "#         print(\"e\",e)\n",
    "#         print(\"c\",c)\n",
    "\n",
    "        #print(flat_list)\n",
    "\n",
    "\n",
    "        # Get uniques facts \n",
    "\n",
    "        flat_list = [item for sublist in e for item in sublist]\n",
    "\n",
    "        flat_set = set(flat_list)\n",
    "        #print(flat_set)\n",
    "        flat_ids = []\n",
    "\n",
    "        for item in flat_set:\n",
    "            for i,(key,value) in enumerate(fact_dict[lang].items()):\n",
    "                if value==item:\n",
    "                    #print(value)\n",
    "                    flat_ids.append(key)\n",
    "        #print(flat_ids)\n",
    "\n",
    "\n",
    "        # Get ids\n",
    "        flat_list_cnts = dict(Counter(flat_list))\n",
    "        #print(flat_list_cnts)\n",
    "\n",
    "        ent = e\n",
    "        for i,value in enumerate(ent):\n",
    "            for j,subitem in enumerate(value):\n",
    "                #print(ent[i][j])\n",
    "                for lookupitem in flat_list_cnts.keys():\n",
    "                    #print(lookupitem)\n",
    "                    if subitem == lookupitem and flat_list_cnts[lookupitem]!=1:    \n",
    "                        ent[i][j] = tuple([ent[i][j],lookupitem + '_' + str(flat_list_cnts[lookupitem])])\n",
    "                        flat_list_cnts[lookupitem] -= 1\n",
    "                        #print(flat_list_cnts)\n",
    "                        #print(e)\n",
    "                        continue\n",
    "                #print(type(ent[i][j]))\n",
    "                if not isinstance(ent[i][j], tuple):\n",
    "                    #print(True)\n",
    "                    ent[i][j] = tuple([ent[i][j],ent[i][j]])\n",
    "\n",
    "\n",
    "        #print(\"Ent\",ent)\n",
    "        flat_ids = []\n",
    "        sent_id=0\n",
    "        act_list = []\n",
    "        flag=0\n",
    "        new_fact_id = 200000\n",
    "        for sent in ent:\n",
    "            for item in sent:\n",
    "                #print(\"item[0]\",item[0])\n",
    "                for i,(key,value) in enumerate(fact_dict[lang].items()):\n",
    "                    if value==item[0]:\n",
    "                        flag==1\n",
    "                        flat_ids.append(key)\n",
    "                        act_list.append(sent_id)\n",
    "                        break\n",
    "#                     flat_ids.append(new_fact_id)\n",
    "#                     new_fact_id +=1\n",
    "#                break\n",
    "\n",
    "            sent_id +=1\n",
    "\n",
    "#        print(\"Actual Clusters\",act_list)\n",
    "#         print(act_list)\n",
    "\n",
    "        ent_perm_list = list(permutations(flat_ids,2))\n",
    "\n",
    "\n",
    "        ent_aff_matrix = np.zeros((len(flat_ids),len(flat_ids)))\n",
    "        #print(ent_aff_matrix.shape)\n",
    "\n",
    "\n",
    "        for p in ent_perm_list:\n",
    "            ent_aff_matrix[flat_ids.index(p[0]),flat_ids.index(p[1])] = affinity_matrix[p[0],p[1]]\n",
    "\n",
    "        #print(ent_aff_matrix)\n",
    "\n",
    "\n",
    "        # clusters\n",
    "        try:\n",
    "            #ent_clusters = cluster.spectral_clustering(affinity=ent_aff_matrix,n_clusters=c)#.fit_predict()\n",
    "            #print(\"Clusters\",ent_clusters)\n",
    "            ent_clusters = cluster.spectral_clustering(affinity=ent_aff_matrix,n_clusters=c)#.fit_predict()\n",
    "            #print(\"predicted Clusters\",ent_clusters)\n",
    "        except Exception:\n",
    "            #nothing +=1\n",
    "            #print(nothing)\n",
    "            #print(\"No clusters\")\n",
    "            #print(ent_aff_matrix.shape,c)\n",
    "            continue\n",
    "\n",
    "\n",
    "        # LABEL MAPPING\n",
    "        if (pd.Series(act_list).nunique() == pd.Series(ent_clusters).nunique()):\n",
    "            new_clusters,cm,old_acc,old_pr,old_re,new_cm,new_acc,new_pr,new_re  = label_matching(act_list,ent_clusters)\n",
    "        else:\n",
    "            #print(\"Missed entity\")\n",
    "            missed +=1\n",
    "            print(\"missed\")\n",
    "\n",
    "        #print(\"New Clusters\",new_clusters)\n",
    "\n",
    "        macro_pr.append(new_pr)\n",
    "        macro_rec.append(new_re)\n",
    "\n",
    "\n",
    "        # Weighted accuracy\n",
    "        #print(new_acc)\n",
    "        weighted_acc.append(new_acc * len(flat_ids))\n",
    "        facts_len+= len(flat_ids)\n",
    "\n",
    "        macro_pr_old.append(old_pr)\n",
    "        macro_rec_old.append(old_re)\n",
    "\n",
    "\n",
    "        entpred_dict[v] = new_clusters\n",
    "\n",
    "    scores['missed_ent'] = missed\n",
    "    macro_pr_total = pd.Series(macro_pr).sum()/cnt\n",
    "    macro_rec_total = pd.Series(macro_rec).sum()/cnt\n",
    "    weighted_acc_total = pd.Series(weighted_acc).sum()/facts_len\n",
    "    macro_pr_old_total = pd.Series(macro_pr_old).sum()/cnt\n",
    "    macro_rec_old_total = pd.Series(macro_rec_old).sum()/cnt\n",
    "\n",
    "    scores['macro_pr'] = round(macro_pr_total*100,2)\n",
    "    scores['macro_rec'] = round(macro_rec_total*100,2)\n",
    "    scores['weighted_acc'] = round(weighted_acc_total*100,2)\n",
    "    scores['macro_pr_old'] = round(macro_pr_old_total*100,2)\n",
    "    scores['macro_rec_old'] = round(macro_rec_old_total*100,2)\n",
    "    print(scores)\n",
    "    scores_dict[lang] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "20a1b7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'te': defaultdict(None,\n",
       "                         {'missed_ent': 0,\n",
       "                          'macro_pr': 79.16,\n",
       "                          'macro_rec': 78.4,\n",
       "                          'weighted_acc': 64.64,\n",
       "                          'macro_pr_old': 44.66,\n",
       "                          'macro_rec_old': 44.55}),\n",
       "             'gu': defaultdict(None,\n",
       "                         {'missed_ent': 2,\n",
       "                          'macro_pr': 79.34,\n",
       "                          'macro_rec': 78.71,\n",
       "                          'weighted_acc': 64.81,\n",
       "                          'macro_pr_old': 46.04,\n",
       "                          'macro_rec_old': 45.88}),\n",
       "             'as': defaultdict(None,\n",
       "                         {'missed_ent': 2,\n",
       "                          'macro_pr': 79.24,\n",
       "                          'macro_rec': 78.73,\n",
       "                          'weighted_acc': 64.88,\n",
       "                          'macro_pr_old': 44.76,\n",
       "                          'macro_rec_old': 44.75}),\n",
       "             'or': defaultdict(None,\n",
       "                         {'missed_ent': 1,\n",
       "                          'macro_pr': 79.16,\n",
       "                          'macro_rec': 78.45,\n",
       "                          'weighted_acc': 64.99,\n",
       "                          'macro_pr_old': 42.99,\n",
       "                          'macro_rec_old': 42.87}),\n",
       "             'ta': defaultdict(None,\n",
       "                         {'missed_ent': 1,\n",
       "                          'macro_pr': 82.46,\n",
       "                          'macro_rec': 81.94,\n",
       "                          'weighted_acc': 67.21,\n",
       "                          'macro_pr_old': 51.74,\n",
       "                          'macro_rec_old': 51.66}),\n",
       "             'bn': defaultdict(None,\n",
       "                         {'missed_ent': 1,\n",
       "                          'macro_pr': 78.55,\n",
       "                          'macro_rec': 77.6,\n",
       "                          'weighted_acc': 63.1,\n",
       "                          'macro_pr_old': 46.93,\n",
       "                          'macro_rec_old': 46.8}),\n",
       "             'hi': defaultdict(None,\n",
       "                         {'missed_ent': 2,\n",
       "                          'macro_pr': 79.54,\n",
       "                          'macro_rec': 78.76,\n",
       "                          'weighted_acc': 64.15,\n",
       "                          'macro_pr_old': 49.77,\n",
       "                          'macro_rec_old': 49.72}),\n",
       "             'pa': defaultdict(None,\n",
       "                         {'missed_ent': 1,\n",
       "                          'macro_pr': 79.19,\n",
       "                          'macro_rec': 78.49,\n",
       "                          'weighted_acc': 63.95,\n",
       "                          'macro_pr_old': 49.98,\n",
       "                          'macro_rec_old': 49.94}),\n",
       "             'en': defaultdict(None,\n",
       "                         {'missed_ent': 5,\n",
       "                          'macro_pr': 80.32,\n",
       "                          'macro_rec': 79.58,\n",
       "                          'weighted_acc': 64.98,\n",
       "                          'macro_pr_old': 55.54,\n",
       "                          'macro_rec_old': 55.51}),\n",
       "             'mr': defaultdict(None,\n",
       "                         {'missed_ent': 0,\n",
       "                          'macro_pr': 80.72,\n",
       "                          'macro_rec': 80.03,\n",
       "                          'weighted_acc': 65.5,\n",
       "                          'macro_pr_old': 56.64,\n",
       "                          'macro_rec_old': 56.63}),\n",
       "             'ml': defaultdict(None,\n",
       "                         {'missed_ent': 3,\n",
       "                          'macro_pr': 80.51,\n",
       "                          'macro_rec': 79.83,\n",
       "                          'weighted_acc': 65.33,\n",
       "                          'macro_pr_old': 55.96,\n",
       "                          'macro_rec_old': 55.91}),\n",
       "             'kn': defaultdict(None,\n",
       "                         {'missed_ent': 5,\n",
       "                          'macro_pr': 80.43,\n",
       "                          'macro_rec': 79.76,\n",
       "                          'weighted_acc': 65.27,\n",
       "                          'macro_pr_old': 55.73,\n",
       "                          'macro_rec_old': 55.7})})"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "7ffb9479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   language total_ent total_sent_all  macro_pr  macro_rec  weighted_acc  \\\n",
      "0        te       664           2459     79.16      78.40         64.64   \n",
      "1        gu       212            706     79.34      78.71         64.81   \n",
      "2        as       189            818     79.24      78.73         64.88   \n",
      "3        or       411           1691     79.16      78.45         64.99   \n",
      "4        ta      2356           6548     82.46      81.94         67.21   \n",
      "5        bn      3492          13285     78.55      77.60         63.10   \n",
      "6        hi      2178           5750     79.54      78.76         64.15   \n",
      "7        pa      1282           4024     79.19      78.49         63.95   \n",
      "8        en      7554          18003     80.32      79.58         64.98   \n",
      "9        mr      1270           2506     80.72      80.03         65.50   \n",
      "10       ml      1966           6343     80.51      79.83         65.33   \n",
      "11       kn       476           1760     80.43      79.76         65.27   \n",
      "\n",
      "    macro_pr_old  macro_rec_old  missed_ent  \n",
      "0          44.66          44.55         0.0  \n",
      "1          46.04          45.88         2.0  \n",
      "2          44.76          44.75         2.0  \n",
      "3          42.99          42.87         1.0  \n",
      "4          51.74          51.66         1.0  \n",
      "5          46.93          46.80         1.0  \n",
      "6          49.77          49.72         2.0  \n",
      "7          49.98          49.94         1.0  \n",
      "8          55.54          55.51         5.0  \n",
      "9          56.64          56.63         0.0  \n",
      "10         55.96          55.91         3.0  \n",
      "11         55.73          55.70         5.0  \n"
     ]
    }
   ],
   "source": [
    "cols  =['language','total_ent','total_sent_all','macro_pr','macro_rec','weighted_acc','macro_pr_old','macro_rec_old']\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i,lang in enumerate(scores_dict):\n",
    "    #print(lang)\n",
    "    df2 = [{'language': lang, 'total_ent':len(ent_dict[lang]),\n",
    "            'total_sent_all':cnts_dict_ent[lang],\n",
    "#           'total_sent':scores_dict[lang]['total_sent'],\n",
    "#            'total_sent_cov':round(scores_dict[lang]['total_sent']/cnts_dict_ent[lang],2),\n",
    "        'missed_ent':scores_dict[lang]['missed_ent'],\n",
    "       'macro_pr':scores_dict[lang]['macro_pr'],\n",
    "        'macro_rec':scores_dict[lang]['macro_rec'],\n",
    "        'weighted_acc':scores_dict[lang]['weighted_acc']\n",
    "        ,'macro_pr_old':scores_dict[lang]['macro_pr_old']\n",
    "        ,'macro_rec_old':scores_dict[lang]['macro_rec_old']}]\n",
    "    #print(df2)\n",
    "    df2= pd.DataFrame.from_dict(df2,orient='columns')\n",
    "    #print(df2)\n",
    "    \n",
    "    df = pd.concat([df,df2], ignore_index=True)\n",
    "    #df.append(df2)\n",
    "    \n",
    "print(df)    \n",
    "\n",
    "df.to_csv(\"../results/clustering-precisionrecall-alllang.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "24519fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision (0.9090909090909091, 0.37878787878787873)\n",
      "Macro Recall (0.9090909090909091, 0.37878787878787873)\n",
      "Weighted_acc 1.0\n"
     ]
    }
   ],
   "source": [
    "# macro_pr = pd.Series(macro_pr).sum()/cnt\n",
    "# macro_rec = pd.Series(macro_rec).sum()/cnt\n",
    "# weighted_acc = pd.Series(weighted_acc).sum()/facts_len\n",
    "# macro_pr_old = pd.Series(macro_pr_old).sum()/cnt\n",
    "# macro_rec_old = pd.Series(macro_rec_old).sum()/cnt\n",
    "# print(\"Macro Precision\",(macro_pr,macro_pr_old))\n",
    "# print(\"Macro Recall\",(macro_rec,macro_rec_old))\n",
    "# print(\"Weighted_acc\",weighted_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae191a10",
   "metadata": {},
   "source": [
    "##### Store validation cluster predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3e9e26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entpred_dict_all['te']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "609edb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['te', 'gu', 'as', 'or', 'ta', 'bn', 'hi', 'pa', 'en', 'mr', 'ml', 'kn'])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entpred_dict_all.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ceedadeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/rahulmehta/Desktop/MultiSent/datasets/cluster_predictions/\"\n",
    "with open(root + 'cluster_predictions_all.pkl', 'wb') as f:\n",
    "    pickle.dump(entpred_dict_all, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4843238c",
   "metadata": {},
   "source": [
    "##### Accuracy on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "eaf86f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(entpred_dict_all['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "363adcd5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# scores_dict = defaultdict()\n",
    "# #l = ['te']\n",
    "# for i,lang in enumerate(entpred_dict_all):\n",
    "#     #print(i)\n",
    "#     print(lang)\n",
    "    \n",
    "#     total_sent = 0\n",
    "#     total_right = 0 \n",
    "#     total_ones = 0\n",
    "#     total_gtpart = 0\n",
    "#     total_gt1sent = 0\n",
    "#     acc_dict = {}\n",
    "#     ent_sent_correct = 0\n",
    "    \n",
    "#     scores = defaultdict()\n",
    "    \n",
    "#     for j,(k,v) in enumerate(entpred_dict_all[lang].items()):\n",
    "#         #print(i)\n",
    "#         #print(k)\n",
    "#         act = ent_dict[lang][k]\n",
    "\n",
    "#     #     print(\"Pred\",k,v)\n",
    "#     #     print(\"Act\",act)\n",
    "\n",
    "        \n",
    "#         num_sent = len(v)\n",
    "#         total_sent += num_sent\n",
    "#         #print(num_sent)\n",
    "\n",
    "#         pred = (v)\n",
    "#         right=0;ones=0\n",
    "#         gtpart_right=0\n",
    "\n",
    "#         # only one sentence and fact\n",
    "#         if num_sent==1: #and len(v[0])==1:\n",
    "#             ones+=1\n",
    "#             #right+=1\n",
    "#             #print(pred)\n",
    "            \n",
    "#         else :\n",
    "#             total_gt1sent +=num_sent\n",
    "#             for sent in pred:\n",
    "#                 for act_sent in act:\n",
    "#                     if sorted(sent) == sorted(act_sent):\n",
    "#                         #print(sorted(sent),sorted(act_sent))\n",
    "#                         right +=1\n",
    "#                         #print(right)\n",
    "#                         break\n",
    "\n",
    "\n",
    "#         #acc = right/num_sent\n",
    "#         total_right += right\n",
    "\n",
    "\n",
    "#         # Check partial or fully matched sentences \n",
    "#         d = np.zeros(len(act))\n",
    "\n",
    "\n",
    "#         for sent in pred:\n",
    "            \n",
    "#             for i,act_sent in enumerate(act):\n",
    "#                 if d[i] == 1:\n",
    "#                     continue\n",
    "\n",
    "\n",
    "#                 if(set(sent).issubset(set(act_sent))):\n",
    "#                     d[i] = 1\n",
    "#                     #print(\"partial match\",sorted(sent),sorted(act_sent))\n",
    "\n",
    "#                     gtpart_right +=1\n",
    "#                     break\n",
    "\n",
    "#         total_gtpart += gtpart_right                \n",
    "#         #print(gtpart_right,total_gtpart) \n",
    "\n",
    "#         total_ones += ones\n",
    "\n",
    "        \n",
    "#         if right == num_sent:\n",
    "#             ent_sent_correct +=1\n",
    "            \n",
    "        \n",
    "#     #print(i)       \n",
    "#     scores['total_sent'] = total_sent\n",
    "#     scores['total_right'] = total_right\n",
    "#     scores['total_ones'] = total_ones\n",
    "#     scores['sent_gt1'] = total_gt1sent\n",
    "#     scores['total_gtpart'] = total_gtpart\n",
    "#     scores['ent_sent_correct'] = ent_sent_correct\n",
    "#     scores['ent'] = j+1\n",
    "    \n",
    "#     print(total_sent)  \n",
    "#     print(total_right)\n",
    "#     print(total_ones)\n",
    "#     print(total_gtpart)\n",
    "    \n",
    "#     scores['sub_acc']  = round(total_gtpart/total_sent,2)\n",
    "#     scores['acc'] = round(total_right/total_gt1sent,2)\n",
    "#     scores['ent_pc_correct'] = round(ent_sent_correct/(j+1),2)\n",
    "#     scores_dict[lang] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "5d614591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'total_sent': 1496,\n",
       "             'total_right': 529,\n",
       "             'total_ones': 140,\n",
       "             'sent_gt1': 1356,\n",
       "             'total_gtpart': 1169,\n",
       "             'ent_sent_correct': 64,\n",
       "             'ent': 536,\n",
       "             'sub_acc': 0.78,\n",
       "             'acc': 0.39,\n",
       "             'ent_pc_correct': 0.12})"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_dict['te']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "973619a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'total_sent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [211], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39mcols)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,lang \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(scores_dict):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#print(lang)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     df2 \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m: lang, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_ent\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28mlen\u001b[39m(ent_dict[lang]),\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_sent_all\u001b[39m\u001b[38;5;124m'\u001b[39m:cnts_dict_ent[lang],\n\u001b[0;32m----> 8\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_sent\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[43mscores_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_sent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_sent_cov\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28mround\u001b[39m(scores_dict[lang][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_sent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m/\u001b[39mcnts_dict_ent[lang],\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     10\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msent_gt1\u001b[39m\u001b[38;5;124m'\u001b[39m:scores_dict[lang][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msent_gt1\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_right\u001b[39m\u001b[38;5;124m'\u001b[39m:scores_dict[lang][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_right\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy_abs_match\u001b[39m\u001b[38;5;124m'\u001b[39m:scores_dict[lang][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m        ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc_sub_match\u001b[39m\u001b[38;5;124m'\u001b[39m:scores_dict[lang][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m         ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ment\u001b[39m\u001b[38;5;124m'\u001b[39m:scores_dict[lang][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ment\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m         ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ment_sent_correct\u001b[39m\u001b[38;5;124m'\u001b[39m:scores_dict[lang][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ment_sent_correct\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m         ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ment_pc_correct\u001b[39m\u001b[38;5;124m'\u001b[39m:scores_dict[lang][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ment_pc_correct\u001b[39m\u001b[38;5;124m'\u001b[39m]}]\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#print(df2)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     df2\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(df2,orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'total_sent'"
     ]
    }
   ],
   "source": [
    "cols  =['language','total_ent','total_sent_all','total_sent','total_sent_cov','sent_gt1','total_right','accuracy_abs_match','acc_sub_match','ent','ent_sent_correct','ent_pc_correct']\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i,lang in enumerate(scores_dict):\n",
    "    #print(lang)\n",
    "    df2 = [{'language': lang, 'total_ent':len(ent_dict[lang]),\n",
    "            'total_sent_all':cnts_dict_ent[lang],\n",
    "            'total_sent':scores_dict[lang]['total_sent'],\n",
    "            'total_sent_cov':round(scores_dict[lang]['total_sent']/cnts_dict_ent[lang],2),\n",
    "       'sent_gt1':scores_dict[lang]['sent_gt1'],\n",
    "        'total_right':scores_dict[lang]['total_right'],\n",
    "        'accuracy_abs_match':scores_dict[lang]['acc']\n",
    "       ,'acc_sub_match':scores_dict[lang]['sub_acc']\n",
    "        ,'ent':scores_dict[lang]['ent']\n",
    "        ,'ent_sent_correct':scores_dict[lang]['ent_sent_correct']\n",
    "        ,'ent_pc_correct':scores_dict[lang]['ent_pc_correct']}]\n",
    "    #print(df2)\n",
    "    df2= pd.DataFrame.from_dict(df2,orient='columns')\n",
    "    #print(df2)\n",
    "    \n",
    "    df = pd.concat([df,df2], ignore_index=True)\n",
    "    #df.append(df2)\n",
    "    \n",
    "df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "1895234b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>total_ent</th>\n",
       "      <th>total_sent_all</th>\n",
       "      <th>macro_pr</th>\n",
       "      <th>macro_rec</th>\n",
       "      <th>weighted_acc</th>\n",
       "      <th>macro_pr_old</th>\n",
       "      <th>macro_rec_old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>te</td>\n",
       "      <td>664</td>\n",
       "      <td>2459</td>\n",
       "      <td>99.55</td>\n",
       "      <td>99.55</td>\n",
       "      <td>100.0</td>\n",
       "      <td>45.23</td>\n",
       "      <td>45.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gu</td>\n",
       "      <td>212</td>\n",
       "      <td>706</td>\n",
       "      <td>99.20</td>\n",
       "      <td>99.20</td>\n",
       "      <td>100.0</td>\n",
       "      <td>46.35</td>\n",
       "      <td>46.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>as</td>\n",
       "      <td>189</td>\n",
       "      <td>818</td>\n",
       "      <td>98.78</td>\n",
       "      <td>98.78</td>\n",
       "      <td>100.0</td>\n",
       "      <td>44.58</td>\n",
       "      <td>44.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>or</td>\n",
       "      <td>411</td>\n",
       "      <td>1691</td>\n",
       "      <td>98.98</td>\n",
       "      <td>98.98</td>\n",
       "      <td>100.0</td>\n",
       "      <td>42.70</td>\n",
       "      <td>42.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ta</td>\n",
       "      <td>2356</td>\n",
       "      <td>6548</td>\n",
       "      <td>99.56</td>\n",
       "      <td>99.56</td>\n",
       "      <td>100.0</td>\n",
       "      <td>51.59</td>\n",
       "      <td>51.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bn</td>\n",
       "      <td>3492</td>\n",
       "      <td>13285</td>\n",
       "      <td>99.74</td>\n",
       "      <td>99.74</td>\n",
       "      <td>100.0</td>\n",
       "      <td>46.68</td>\n",
       "      <td>46.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hi</td>\n",
       "      <td>2178</td>\n",
       "      <td>5750</td>\n",
       "      <td>99.71</td>\n",
       "      <td>99.71</td>\n",
       "      <td>100.0</td>\n",
       "      <td>49.70</td>\n",
       "      <td>49.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pa</td>\n",
       "      <td>1282</td>\n",
       "      <td>4024</td>\n",
       "      <td>99.65</td>\n",
       "      <td>99.65</td>\n",
       "      <td>100.0</td>\n",
       "      <td>49.66</td>\n",
       "      <td>49.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>en</td>\n",
       "      <td>7554</td>\n",
       "      <td>18003</td>\n",
       "      <td>99.75</td>\n",
       "      <td>99.75</td>\n",
       "      <td>100.0</td>\n",
       "      <td>55.28</td>\n",
       "      <td>55.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mr</td>\n",
       "      <td>1270</td>\n",
       "      <td>2506</td>\n",
       "      <td>99.76</td>\n",
       "      <td>99.76</td>\n",
       "      <td>100.0</td>\n",
       "      <td>56.43</td>\n",
       "      <td>56.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ml</td>\n",
       "      <td>1966</td>\n",
       "      <td>6343</td>\n",
       "      <td>99.75</td>\n",
       "      <td>99.75</td>\n",
       "      <td>100.0</td>\n",
       "      <td>55.86</td>\n",
       "      <td>55.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>kn</td>\n",
       "      <td>476</td>\n",
       "      <td>1760</td>\n",
       "      <td>99.72</td>\n",
       "      <td>99.72</td>\n",
       "      <td>100.0</td>\n",
       "      <td>55.60</td>\n",
       "      <td>55.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   language total_ent total_sent_all  macro_pr  macro_rec  weighted_acc  \\\n",
       "0        te       664           2459     99.55      99.55         100.0   \n",
       "1        gu       212            706     99.20      99.20         100.0   \n",
       "2        as       189            818     98.78      98.78         100.0   \n",
       "3        or       411           1691     98.98      98.98         100.0   \n",
       "4        ta      2356           6548     99.56      99.56         100.0   \n",
       "5        bn      3492          13285     99.74      99.74         100.0   \n",
       "6        hi      2178           5750     99.71      99.71         100.0   \n",
       "7        pa      1282           4024     99.65      99.65         100.0   \n",
       "8        en      7554          18003     99.75      99.75         100.0   \n",
       "9        mr      1270           2506     99.76      99.76         100.0   \n",
       "10       ml      1966           6343     99.75      99.75         100.0   \n",
       "11       kn       476           1760     99.72      99.72         100.0   \n",
       "\n",
       "    macro_pr_old  macro_rec_old  \n",
       "0          45.23          45.23  \n",
       "1          46.35          46.35  \n",
       "2          44.58          44.58  \n",
       "3          42.70          42.70  \n",
       "4          51.59          51.59  \n",
       "5          46.68          46.68  \n",
       "6          49.70          49.70  \n",
       "7          49.66          49.66  \n",
       "8          55.28          55.28  \n",
       "9          56.43          56.43  \n",
       "10         55.86          55.86  \n",
       "11         55.60          55.60  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "c56e572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../results/clustering-scores-alllang-occ.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
