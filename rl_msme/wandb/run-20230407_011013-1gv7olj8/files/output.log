
[38mic| model_gpus: [0]
[38mic| next(model.parameters()).is_cuda: True
[38mic| start_epoch: 0, final_checkpoint: None
  0%|                                                                                                                                                    | 0/5 [00:00<?, ?it/s, loss=0]

[38m    greedy_idx.shape: torch.Size([2, 41])[39m                                                                                                    | 0/33932 [00:00<?, ?it/s, model_number=0]
[38mic| self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True): ['तार उल्लेखयोग्य रचनागुलो हल " द्य रीम अब द्य अ्यान्टिनोपिआ, बाय़ोग्राफिआ '
[38m                                                                           'लरिय़ेटरी ", " द्य रीम अब द्य अ्यान्टिनोपिआ, कुबला खान ", " द्य रीम अब द्य '
[38m                                                                           'अ्यान्टिनोपिआ, द्य रीम अब द्य अ्यान्टिनोपिआ, द्य रीम अब द्य अ्यान्टिनोपिआ, '
[38m                                                                           'द्य रीम अब द्य अ्यान्टिनोपिआ, द्य रीम अब द्य अ्यान्टिनोपिआ, द्य रीम अब द्य '
[38m                                                                           'अ्यान्टिनोपिआ, द्य रीम अब द्य अ्यान्टिनोपिआ, बाय़ोग्राफिआ लरिय़ेटरीआ, द्य '
[38m                                                                           'रीम अब द्य अ्यान्टिनोपिआ, द्य रीम अब द्य अ्यान्टिनो',
[38m                                                                           'सुनील वर्मा ( जन्म 28 फेब्रुय़ारि 1974 ), सुनील वर्मा नामे अधिक परिचित, एक '
[38m                                                                           'भारतीय़ अभिनेता, यिनि तेलुगु, तामिल, तेलुगु, तेलुगु, तामिल, तेलुगु, तेलुगु, '
[38m                                                                           'तेलुगु, तामिल, तेलुगु, तेलुगु, तेलुगु, तेलुगु, तामिल, तेलुगु, तेलुगु, '
[38m                                                                           'तेलुगु, तेलुगु, तेलुगु, तामिल, तेलुगु, तेलुगु, तेलुगु, तेलुगु, तेलुगु, '
[38m                                                                           'तामिल, तेलुगु, तेलुगु, तेलुगु, तेलुगु, तेलुगु, तामिल, तेलुगु, तेलुगु, '
[38m                                                                           'तेलुगु, तेलुगु ']
[38mic| self.tokenizer.batch_decode(greedy_idx, skip_special_tokens=True): [' is  s "  Rim of of the Ancient Mariner " ( " कुla Khan ", "  well as " "   '
[38m                                                                        '" कुBiographia Literaria ", "',
[38m                                                                        'सुनil Varma (  28 फरFebruary 1974 )  an Indian अभि   pre Telugu ,   सुन सुन '
[38m                                                                        'सुन सुन सुन सुन सुन सुन सुन सुन सुन सुन सुन सुन सुन']
  0%|                                                                                                                                        | 0/33932 [00:04<?, ?it/s, model_number=0]
  0%|                                                                                                                                                    | 0/5 [00:04<?, ?it/s, loss=0]
Traceback (most recent call last):
  File "train_ddp.py", line 439, in <module>
    main()
  File "train_ddp.py", line 381, in main
    total_loss.backward()
  File "/home2/aditya_hari/miniconda3/envs/multisent/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/home2/aditya_hari/miniconda3/envs/multisent/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 490.00 MiB (GPU 0; 10.76 GiB total capacity; 3.04 GiB already allocated; 437.69 MiB free; 3.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF